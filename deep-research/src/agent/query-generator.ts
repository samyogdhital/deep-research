// IMPORTANT: Do not remove the comment at all.
// Step 1: This query generator(master) agent will initially get the user's initial detailed prompt and some set of followup questions along with answers to that questions. So that agent can understand the user's question with great clarity. Now this agent must understand the exact the query user is trying to understand and learn based on the information query generator agent got. Since the user will give the agent his understanding as well as the focus of the research the user wants to do in that initial query and followup questions, the agent need to recognize the user's understanding level.
// Step 2: Then the agent need to generate very precise keywords to do serp query. These keywords must be 1-7 words long. No more words beyond 7. These queries needs to be highly precise and strategic to get the most relevant information from the internet. The agent must understand and generate queries, if hit on search engine, will very very precisely answer the exact query the user is asking. Along with the query, a very detailed long objective is also generated by the query generator agent under each serp query. That means, the agent will generate serp query with very detailed and precise objective. The serp query and the objective should make sure that, if the query is hit on the search engine and say top 10 websites results coming from the search engine, if we scrape those websites, the objective of the information is 100% met. Before generating the objective and query, the agent must make sure it understands the user query precisely.

import { Schema, SchemaType } from '@google/generative-ai';

import { callGeminiLLM, SystemInstruction, UserPrompt } from '../ai/providers';
import { DBSchema } from '../db/db';
import { DBResearchData } from './report-writer';

export interface QueryWithObjective {
    query: string;
    objective: string;
    query_timestamp: number;
    parent_query_timestamp: number;
}

const QUERY_SCHEMA: Schema = {
    type: SchemaType.OBJECT,
    properties: {
        queries: {
            type: SchemaType.ARRAY,
            items: {
                type: SchemaType.OBJECT,
                properties: {
                    query: {
                        type: SchemaType.STRING,
                        description:
                            "Highly relevent very specific and precise keyword to do serp query. This serp query will besued to get the website list, we then scrape the websites and get 100% precise answer to the user's question. That's why you must generate query that will 100% hit on the search engine and get the most relevant websites. This must be 4-15 word max and should only use plain english.",
                    },
                    objective: {
                        type: SchemaType.STRING,
                        description:
                            'The highly detailed precise objective of the query that we generated for websites analyzing agent to scrape on and analyze whether this objective is met or not analyzing website content. This must as descriptive as possible. Ideally, it must be 5-10 sentences long highly technical and highly detailed.',
                    },
                },
                required: ['query', 'objective'],
            },
        },
    },
    required: ['queries'],
};

export async function generateQueriesWithObjectives(
    db_research_data: DBResearchData,
    current_depth_level: number,
    numQueries: number,
    parent_query_timestamp?: number
): Promise<QueryWithObjective[]> {
    // Validate parent_query_timestamp for depth > 1
    if (current_depth_level > 1 && !parent_query_timestamp) {
        throw new Error('parent_query_timestamp is required for depth > 1');
    }

    const structuredPrompt = generateContextforSerpQueryGeneratorAgent(
        db_research_data,
        current_depth_level,
        parent_query_timestamp || 0 // Safe to pass 0 for depth 1 as it won't be used
    );

    const systemInstruction: SystemInstruction = {
        role: 'system',
        parts: [
            {

                text: `
    You are the Query Generator Agent, tasked with creating highly precise search queries to retrieve websites that contain the exact information needed to fully answer the user's research question in great technical detail. Your role is critical: the queries you generate will be searched on various search engines (e.g., Google, Bing, or others) to obtain a list of websites, which will then be scraped and analyzed to extract content that directly addresses the user's question. Your queries must guarantee that the top results from any search engine will 100% contain the relevant information.

    **Input Context:**
    You will receive:
    - The user's initial research prompt.
    - Follow-up questions and answers provided by the user.
    - For research depths greater than 1, the learnings from previous queries, including past queries, objectives, and extracted content from scraped websites.

    Use this context to deeply understand the user's intent, the scope of their question, and any specific nuances or focus areas they've highlighted.

    **Query Generation Guidelines:**
    - **Length**: Each query must be 4 to 15 words long.
    - **Language**: Use plain English, avoiding jargon unless it's critical to the topic and likely to appear in relevant websites.
    - **Precision**: Craft queries that are highly specific and targeted, using keywords, phrases, names, dates, or technical terms that directly relate to the user's question.
    - **Search Engine Compatibility**: Ensure queries work effectively across all search engines. You may use basic operators like quotes ("") for exact phrases to enhance precision, but avoid engine-specific syntax.
    - **100% Relevance**: Design each query so that the top websites returned are guaranteed to contain the precise information needed. Avoid broad or vague terms that could retrieve unrelated results.
    - **Diversity**: When generating multiple queries, ensure each one addresses a unique aspect, subtopic, or nuance of the user's question. Avoid overlap with other queries in the set or with previous queries from the parent chain (at deeper levels).
    - **Step-by-Step Reasoning**: Before generating queries, analyze the context step-by-step:
    1. Identify the core topic and key information needs from the initial prompt.
    2. Review follow-up Q&A to pinpoint specific details or clarifications the user seeks.
    3. For deeper levels, examine previous learnings to identify gaps, unanswered questions, or areas for deeper exploration.
    4. Break the topic into distinct subtopics or angles to cover comprehensively with the specified number of queries.

    **Objective Generation Guidelines:**
    - **Detail**: Each objective must be 5-10 sentences long, highly technical, and extremely descriptive.
    - **Clarity**: Specify exactly what information the Website Analysis Agent should extract from the websites returned by the query. Include details like time frames, technical specifications, or specific aspects of interest.
    - **Precision**: Ensure the objective aligns perfectly with the query, defining the criteria for success so the Website Analysis Agent can confirm whether the website content meets the goal.
    - **Contextual Relevance**: Base the objective solely on the provided context, reflecting the user's intent and the specific subtopic the query targets.

    **For Deeper Research Levels (Depth > 1):**
    - Use the learnings from previous queries to refine your approach.
    - Focus on gaps in knowledge, unresolved questions, or promising areas identified in prior results.
    - Ensure new queries build on past findings without duplicating previous efforts.

    **Output Requirements:**
    - Generate exactly the number of queries specified in the user prompt (e.g., [numQueries]).
    - Provide your response as a valid JSON object with the following structure:
    {
    "queries": [
        {
        "query": "string",
        "objective": "string"
        },
        ...
    ]
    }
  
    Include no additional text, explanations, or comments outside the JSON object.

    Base all queries and objectives solely on the provided context, without introducing external knowledge or assumptions.

    Current Date: Today is ${new Date().toISOString()}. Use this to ensure queries and objectives are time-relevant if the user’s question involves recent information.

`,
            }
        ]
    }

    const userPrompt = [{
        text: `
    ### User Prompt
    Using the research context provided below, generate exactly [numQueries] search queries and their corresponding objectives to retrieve websites that will 100% contain the precise, detailed answers to the user’s research question.

    CONTEXT:
    ${structuredPrompt}

    Instructions:
    Each query must be a 4-15 word string in plain English, designed to return websites from any search engine (e.g., Google, Bing) that directly address a specific part of the user’s question.

    Each objective must be a 5-10 sentence, highly technical description of the exact information to extract from the websites returned by the query.

    Ensure the ${numQueries} queries collectively cover all key aspects, subtopics, and nuances of the user’s question as outlined in the context.

    For deeper research levels, use previous learnings to target unexplored areas or deeper details, avoiding overlap with past queries.

    Output your response as a JSON object:
    {
    "queries": [
        {
        "query": "string",
        "objective": "string"
        },
        ...
    ]
    }

    Do not include any text outside the JSON object.

        `
    }]

    const userPromptSchema: UserPrompt = {
        contents: [{ role: 'user', parts: userPrompt }],
    };


    try {
        const { response } = await callGeminiLLM({
            system: systemInstruction,
            user: userPromptSchema,
            model: process.env.QUERY_GENERATING_MODEL as string,
            generationConfig: {
                responseSchema: QUERY_SCHEMA,
            },
        });

        const result: { queries: Pick<QueryWithObjective, "query" | "objective">[] } = JSON.parse(response.text());

        if (
            !result?.queries ||
            !Array.isArray(result.queries) ||
            result.queries.length === 0
        ) {
            throw new Error('No valid queries generated');
        }

        return result.queries.map((query, index: number) => ({
            ...query,
            query_timestamp: Date.now() + index,
            parent_query_timestamp: parent_query_timestamp || 0
        }));


    } catch (error) {
        console.error('Error generating queries:', error);
        throw error;
    }
}


type ParentChains = DBSchema['researches'][number]['serpQueries']

const generateContextforSerpQueryGeneratorAgent = (
    db_research_data: DBResearchData,
    current_depth_level: number,
    parent_query_timestamp: number
) => {
    // Get the parent chain first
    const parentChain = buildParentChain(db_research_data, current_depth_level, parent_query_timestamp);

    return `

  Here is the initial prompt the user gave us to do deep research on:
  "${db_research_data.initial_query}"

  Here are all the followup questions and answers the user gave us:
  ${db_research_data.followUps_QnA
            .map(
                ({ question, answer }) => `
  - Question: "${question}"
        Answer: "${answer}"
  `,
            )
            .join('\n')}

  ${current_depth_level > 1 ? `
  Here are all the learning that we got from the previous serp queries:
  ${parentChain
                .map(
                    (
                        parentQuery
                    ) => `
===============================================
===============================================
  
  
  ${((depth_level: number) => {
                            if (depth_level === 1) return 'HIGHEST TOP LEVEL PARENT';
                            if (depth_level === 2) return '2nd LEVEL';
                            if (depth_level === 3) return '3rd LEVEL';
                            return `${depth_level}th`;
                        })(parentQuery.depth_level)} DEPTH QUERY'S INFORMATION:
      1. Serp Query: "${parentQuery.query}"
      2. Depth of this query: "${parentQuery.depth_level}"
      3. Objective: "${parentQuery.objective}"
      ${parentQuery.depth_level > 1
                            ? `
      INFORMATION ABOUT ITS PARENT QUERY
      1. Parent Query: "${db_research_data.serpQueries.find(x => x.query_timestamp === parentQuery.parent_query_timestamp)?.query}"
      2. Parent Objective: "${db_research_data.serpQueries.find(x => x.query_timestamp === parentQuery.parent_query_timestamp)?.objective}"
      3. Parent Depth Level: "${db_research_data.serpQueries.find(x => x.query_timestamp === parentQuery.parent_query_timestamp)?.depth_level}"
      `
                            : ``
                        }
  
  LEARNINGS WE GOT FROM SCRAPING ALL THE WEBSITES UNDER THIS SERP QUERY:
  Here are the list of serp queries from top level parent query to the direct parent query of this query. We have scraped all the websites under these queries and got the following learning. Based on this learning, we will generate the new serp query for the next depth:
  ${parentQuery.successful_scraped_websites.map(x => `
      1. Website: "${x.url}"
      2. Is objective met from this website?: "${x.is_objective_met}"
      3. Core extracted Content from this website: \n         - ${x.core_content.join(
                            '\n         - ',
                        )}
      4. Facts and Figures extracted from this website: ${x.facts_figures.join(
                            '\n         - ',
                        )}
      `).join('\n-------------------------------------------------\n')}
  
===============================================
===============================================
      `,
                )
                .join('\n')}` : ``}`;
}

function buildParentChain(
    db_research_data: DBSchema['researches'][number],
    current_depth_level: number,
    parent_query_timestamp: number
): ParentChains {
    // If depth 1, return empty array since it has no parents
    if (current_depth_level === 1) {
        return [];
    }

    const parentChain = [];
    let currentParentTimestamp = parent_query_timestamp;

    // Build chain from current depth's parent back to depth 1
    for (let depth = current_depth_level - 1; depth >= 1; depth--) {
        const parentQuery = db_research_data.serpQueries.find(
            q => q.query_timestamp === currentParentTimestamp
        );

        if (parentQuery) {
            // Add to start to maintain depth order (depth 1 first, then 2, etc.)
            parentChain.unshift(parentQuery);
            currentParentTimestamp = parentQuery.parent_query_timestamp;
        }
    }

    return parentChain;
}


// TODO: Important do not remove this comment at all.

//Depth 1: On top level query and depth 1, we generate serp query only based on initial prompt, followups question and answer given by the user.

// Depth 2: On depth 2, we generate serp query based on the initial prompt, followups question and answer given by the user, as well as the query name, objective and learning we got from depth 1 parent query.

// Depth 3: We get all the infomratios from depth 1, depth 2 and depth 3 and we generate the serp query taking learning from depth 1, depth 2 and depth 3.

// Depth n; We get all the informations from depth 1 to depth n-1 and we generate the serp query taking learning from depth 1 to depth n-1.

// For every depth we analyze, initial prompt given by the user, the followup question and answers given by the user, all the above depth parent queries tracing back to the depth 1 query, we take the query name, objective and the information we extracted from each websites under each query through website analyzer agent taking all of these as the learning to generate the new serp query and objective for the next depth. We learn and then generate the new serp query to fill our knowledge gap for fulfilling the request the user has asked for.

// --------------------------------


// Example output of the context generator for serp query generator agent.

//   Here is the initial prompt the user gave us to do deep research on:
//   "Find detailed technical specifications and design considerations for space-based data centers."

//   Here are all the followup questions and answers the user gave us:
//   - Question: "What are the design considerations for space-based data centers?"
//   - Answer: "The design considerations for space-based data centers are the same as the design considerations for terrestrial data centers, but with some additional considerations due to the harsh environment of space."

//   Here are all the learning that we got from the previous serp queries:

// ===============================================
// ===============================================

// HIGHEST TOP LEVEL PARENT DEPTH QUERY'S INFORMATION:
//     1. Serp Query: "space datacenter architecture design"
//     2. Depth of this query: "1"
//     3. Objective: "Find detailed technical specifications and design considerations for space-based data centers."

// LEARNINGS WE GOT FROM SCRAPING ALL THE WEBSITE UNDER THIS SERP QUERY:

//     1. Website: "https://www.aflhyperscale.com/articles/physical-requirements-for-ai-data-centers/"
//     2. Is objective met from this website?: "true"
//     3. Core extracted Content from this website:
//          - AI data centers are shifting from 5-10kW server racks to designs capable of handling 50-100kW or more, with some facilities reaching 200kW.
//          - Traditional CPU-powered data centers are insufficient for parallel processing tasks, leading to the use of GPUs in AI data centers.
//          - GPUs excel at parallel processing due to a high number of cores, which is essential for machine learning calculations during training and inference.
//          - The number of transistors in a single GPU can be significantly higher than in a CPU, leading to increased physical space requirements.
//          - Increased power consumption and heat generation necessitate advanced cooling solutions in AI data centers.
//          - Traditional air-cooling methods are inadequate for high-density AI data centers, and liquid cooling techniques are being adopted.
//          - Direct-to-chip liquid cooling introduces liquid coolant directly to the chip and is suitable for heat dispersal exceeding 100kW+ per rack.
//          - Rear-door heat exchangers are used for server rack densities ranging from 20 kW to 50 kW per rack, often used to retrofit existing racks.
//          - Immersion cooling involves submerging servers in a non-toxic synthetic liquid and can manage heat loads exceeding 250 kW.
//          - The demand for robust power and cooling infrastructure expands the physical footprint of AI data centers and drives innovation in energy efficiency.
//     4. Facts and Figures extracted from this website: Global data creation and consumption estimates exceeded 64 zettabytes in 2020.
//          - Data creation and consumption predictions are expected to surpass 180 zettabytes by 2025.
//          - Some cutting-edge facilities now accommodate racks reaching 200kW.
//          - The average American home uses approximately 30kWh per day.
//          - Each data center cabinet consumes the same daily energy as 3–6 typical American homes.
//          - Modern GPUs can achieve calculation speeds reaching teraflops (trillions of FLOPS), with some high-end chips reaching petaflops (quadrillions of FLOPS).
//          - The NVIDIA GeForce RTX 3090 performs at up to 35.6 teraflops.
//          - The Intel i7-9700K (CPU) contains three billion transistors.
//          - The Nvidia Blackwell B100 accelerator (GPU) contains 208 billion transistors.
//          - A study revealed that 38.6% of IT professionals expect to see a rise in liquid cooling techniques throughout data centers by 2026.

//  -------------------------------------------------

//     1. Website: "https://www.techtarget.com/searchdatacenter/tip/Projections-and-feasibility-of-data-centers-in-space"
//     2. Is objective met from this website?: "true"
//     3. Core extracted Content from this website:
//          - Space-based data centers could save land use on Earth and reduce energy costs due to solar power technology.
//          - Data centers in Low Earth Orbit (LEO) could provide lower energy costs.
//          - Solar power and batteries would supply all the power in space, reducing running costs.
//          - A spacecraft in LEO orbits Earth roughly every 90 minutes, with about 45 minutes in sunlight and the remaining time in darkness; solar panels maintain the data center's batteries.
//          - Placing a data center in orbit could improve performance for satellite-based workloads by allowing satellites to transmit data at a higher rate and process raw data before sending it to Earth.
//          - Axiom Space and Skyloom are partnering to create the world's first orbital data center, planning to support data transmission speeds of 10 Gbps.
//          - Data centers in LEO would save land space on Earth.
//          - Data centers in LEO would have few physical security threats.
//          - Orbital data centers would communicate with satellites and receiving stations using secure channels and closed communication systems, guarding against cyberattacks.
//          - Space vehicles endure extreme vibrations and high g-force loads during launch, potentially damaging non-solid-state components.
//          - Radiation can cause bit flips and memory corruption, and galactic cosmic rays can destroy transistors; error-correcting memory and shielding against radioactive particles are required.
//          - In space, conventional cooling systems are not as effective due to the lack of convection, but space-based computers use radiators filled with ammonia for cooling.
//          - Orbital data center infrastructure must protect hardware from micrometeorite impacts and temperature fluctuations, utilizing stainless steel or titanium for high-stress components, aluminum alloy for most construction, gold or silver foil for radiation protection, Kevlar and Nextel for impact protection, and an external thermal blanket for insulation.
//          - HPE's experiment on the International Space Station (ISS) demonstrated that off-the-shelf data center hardware could function in space, but with special software to correct errors from radiation.
//          - The space shuttles' General-Purpose Computers (GPCs) functioned similarly to a failover clustering environment for redundancy and error correction.
//     4. Facts and Figures extracted from this website: Data centers in LEO could save land use on Earth.
//          - A spacecraft in LEO orbits Earth roughly every 90 minutes.
//          - Solar panels maintain the data center's batteries.
//          - Axiom Space plans to support data transmission speeds of 10 Gbps.
//          - One data center facility typically occupies about 40 acres of land on Earth.
//          - Hyperscale data centers occupy hundreds of acres.
//          - Data centers in LEO would have few physical security threats.
//          - Space vehicles endure extreme vibrations and high g-force loads during launch.
//          - HPE launched a supercomputer to the ISS in 2017.
//          - HPE's experiment showed nine out of 20 solid-state drives failed during the mission.
//          - HPE launched follow-up missions in 2021 and 2024.
//          - Space shuttles contained five GPCs.
//          - During normal operations, four computers were online at the same time on space shuttles' GPCs.
//          - Each of the four computers in space shuttles' GPCs had a vote.
//          - The fifth GPC on the space shuttles served as a backup flight system.
//          - The article was published on 08 Nov 2024.

//  -------------------------------------------------

//     1. Website: "https://www.techtarget.com/searchdatacenter/tip/Projections-and-feasibility-of-data-centers-in-space"
//     2. Is objective met from this website?: "true"
//     3. Core extracted Content from this website:
//          - Space-based data centers offer solutions for extreme temperature variations.
//          - Conventional cooling systems are ineffective in space due to the absence of convection in microgravity.
//          - Space-based computers, like those on the International Space Station (ISS), use radiators filled with ammonia for cooling.
//          - Orbital data center infrastructure must protect internal hardware from micrometeorite impacts and temperature fluctuations.
//          - High-stress structural components of orbital data centers should primarily be made from stainless steel or titanium.
//          - Aluminum alloy is suggested for most of the construction of an orbital data center.
//          - A layer of gold or silver foil is needed to protect against radiation and for temperature control.
//          - A layer of Kevlar and Nextel can provide protection against impacts.
//          - An external thermal blanket would provide additional insulation against extreme temperatures.
//          - HPE launched a supercomputer to the ISS to test hardware durability and performance.
//          - HPE's Spaceborne Computer utilized special software to correct errors caused by radiation or cosmic rays.
//          - HPE launched follow-up missions to the ISS in 2021 and 2024 to test additional data center hardware.
//     4. Facts and Figures extracted from this website: In space, there can be several hundred degrees of difference in temperature between sunlight and shadows.
//          - Space-based computers, like those on the International Space Station (ISS), use radiators filled with ammonia for cooling.
//          - Space vehicles endure extreme vibrations and high g-force loads during launch.
//          - Radiation can cause bit flips and memory corruption.
//          - Galactic cosmic rays can cause high-energy particles to impact orbital systems, which can destroy or shorten the life of transistors.
//          - Data centers in LEO would save the limited land space left on Earth, leaving it for more efficient use, like farming or housing developments.
//          - Axiom Space has formed a partnership with Skyloom with the goal of creating the world's first orbital data center.
//          - Axiom Space plans to support data transmission speeds of 10 Gbps.
//          - In 2017, HPE launched a supercomputer to the ISS.
//          - Nine out of 20 solid-state drives failed during HPE's mission to the ISS.
//          - The two servers of HPE spent over a year and a half in orbit.
//          - HPE launched follow-up missions, sending additional data center hardware to the ISS in 2021 and 2024.

// ===============================================
// ===============================================
