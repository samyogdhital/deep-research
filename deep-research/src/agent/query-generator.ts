// IMPORTANT: Do not remove the comment at all.
// Step 1: This query generator(master) agent will initially get the user's initial detailed prompt and some set of followup questions along with answers to that questions. So that agent can understand the user's question with great clarity. Now this agent must understand the exact the query user is trying to understand and learn based on the information query generator agent got. Since the user will give the agent his understanding as well as the focus of the research the user wants to do in that initial query and followup questions, the agent need to recognize the user's understanding level.
// Step 2: Then the agent need to generate very precise keywords to do serp query. These keywords must be 1-7 words long. No more words beyond 7. These queries needs to be highly precise and strategic to get the most relevant information from the internet. The agent must understand and generate queries, if hit on search engine, will very very precisely answer the exact query the user is asking. Along with the query, a very detailed long objective is also generated by the query generator agent under each serp query. That means, the agent will generate serp query with very detailed and precise objective. The serp query and the objective should make sure that, if the query is hit on the search engine and say top 10 websites results coming from the search engine, if we scrape those websites, the objective of the information is 100% met. Before generating the objective and query, the agent must make sure it understands the user query precisely.

import { Schema, SchemaType } from '@google/generative-ai';

import { generateObject } from '../ai/providers';
import { DBSchema } from '../db';
import { SerpQueryResult } from '../types';
import { DBResearchData } from './report-writer';

export interface QueryWithObjective {
    query: string;
    objective: string;
}

const QUERY_SCHEMA: Schema = {
    type: SchemaType.OBJECT,
    properties: {
        queries: {
            type: SchemaType.ARRAY,
            items: {
                type: SchemaType.OBJECT,
                properties: {
                    query: {
                        type: SchemaType.STRING,
                        description:
                            "Short Very specific keyword to do serp query, get the website list under each query, scrape the websites and get 100% precise answer to the user's question. This must be 4-5 word max and should only use plain english.",
                    },
                    objective: {
                        type: SchemaType.STRING,
                        description:
                            'The highly detailed precise objective of the query that we generated for websites analyzing agent to scrape on and analyze whether this objective is met or not analyzing website content.',
                    },
                },
                required: ['query', 'objective'],
            },
        },
    },
    required: ['queries'],
};

export async function generateQueriesWithObjectives(
    db_research_data: DBResearchData,
    current_depth_level: number,
    numQueries: number,
    parent_query_timestamp?: number
): Promise<QueryWithObjective[]> {
    // Validate parent_query_timestamp for depth > 1
    if (current_depth_level > 1 && !parent_query_timestamp) {
        throw new Error('parent_query_timestamp is required for depth > 1');
    }

    const structuredPrompt = generateContextforSerpQueryGeneratorAgent(
        db_research_data,
        current_depth_level,
        parent_query_timestamp || 0 // Safe to pass 0 for depth 1 as it won't be used
    );

    console.log("ðŸ”ðŸ”", structuredPrompt, "ðŸ”ðŸ”");
    try {
        const { response } = await generateObject({
            system: `You are the high quality SERP query generating agent. Your role is to analyze the user's query and followup questions list. Then generate a bunch of short very detailed serp queries that in total entirely summarizes the entire question user is asking. Such that if you scrape the list of websites that you get under each of these queires, there is 100% chance that user's question will be 100% precisely answered in great highly technical detail. Today is ${new Date().toISOString()}. Follow these instructions when responding:
    - Your serp query must be short and 100% targeted and precise that can accurately summarizes a domain of the entire question that user is aksing.
    - Use your reasoning ability to understand the user's question and asking. Then only generate the queries.
    `,
            prompt: `
Given this research context, generate ${numQueries} strategic search queries to find the list of websites we can scrape and get the percise and 100% answer to the question user is asking below.

CONTEXT:
${structuredPrompt}
`,
            model: process.env.QUESTION_GENERATING_MODEL as string,
            generationConfig: {
                responseSchema: QUERY_SCHEMA,
            },
        });

        const result = JSON.parse(response.text());

        if (
            !result?.queries ||
            !Array.isArray(result.queries) ||
            result.queries.length === 0
        ) {
            throw new Error('No valid queries generated');
        }

        return result.queries;
    } catch (error) {
        console.error('Error generating queries:', error);
        throw error;
    }
}


type ParentChains = DBSchema['researches'][number]['serpQueries']

const generateContextforSerpQueryGeneratorAgent = (
    db_research_data: DBResearchData,
    current_depth_level: number,
    parent_query_timestamp: number
) => {
    // Get the parent chain first
    const parentChain = buildParentChain(db_research_data, current_depth_level, parent_query_timestamp);

    return `

  Here is the initial prompt the user gave us to do deep research on:
  "${db_research_data.initial_query}"

  Here are all the followup questions and answers the user gave us:
  ${db_research_data.followUps_QnA
            .map(
                ({ question, answer }) => `
  - Question: "${question}"
        Answer: "${answer}"
  `,
            )
            .join('\n')}

  ${current_depth_level > 1 ? `
  Here are all the learning that we got from the previous serp queries:
  ${parentChain
                .map(
                    (
                        parentQuery
                    ) => `
===============================================
===============================================
  
  
  ${((depth_level: number) => {
                            if (depth_level === 1) return 'HIGHEST TOP LEVEL PARENT';
                            if (depth_level === 2) return '2nd LEVEL';
                            if (depth_level === 3) return '3rd LEVEL';
                            return `${depth_level}th`;
                        })(parentQuery.depth_level)} DEPTH QUERY'S INFORMATION:
      1. Serp Query: "${parentQuery.query}"
      2. Depth of this query: "${parentQuery.depth_level}"
      3. Objective: "${parentQuery.objective}"
      ${parentQuery.depth_level > 1
                            ? `
      INFORMATION ABOUT ITS PARENT QUERY
      1. Parent Query: "${db_research_data.serpQueries.find(x => x.query_timestamp === parentQuery.parent_query_timestamp)?.query}"
      2. Parent Objective: "${db_research_data.serpQueries.find(x => x.query_timestamp === parentQuery.parent_query_timestamp)?.objective}"
      3. Parent Depth Level: "${db_research_data.serpQueries.find(x => x.query_timestamp === parentQuery.parent_query_timestamp)?.depth_level}"
      `
                            : ``
                        }
  
  LEARNINGS WE GOT FROM SCRAPING ALL THE WEBSITES UNDER THIS SERP QUERY:
  Here are the list of serp queries from top level parent query to the direct parent query of this query. We have scraped all the websites under these queries and got the following learning. Based on this learning, we will generate the new serp query for the next depth:
  ${parentQuery.successful_scraped_websites.map(x => `
      1. Website: "${x.url}"
      2. Is objective met from this website?: "${x.is_objective_met}"
      3. Core extracted Content from this website: \n         - ${x.core_content.join(
                            '\n         - ',
                        )}
      4. Facts and Figures extracted from this website: ${x.facts_figures.join(
                            '\n         - ',
                        )}
      `).join('\n-------------------------------------------------\n')}
  
===============================================
===============================================
      `,
                )
                .join('\n')}` : ``}`;
}

function buildParentChain(
    db_research_data: DBSchema['researches'][number],
    current_depth_level: number,
    parent_query_timestamp: number
): ParentChains {
    // If depth 1, return empty array since it has no parents
    if (current_depth_level === 1) {
        return [];
    }

    const parentChain = [];
    let currentParentTimestamp = parent_query_timestamp;

    // Build chain from current depth's parent back to depth 1
    for (let depth = current_depth_level - 1; depth >= 1; depth--) {
        const parentQuery = db_research_data.serpQueries.find(
            q => q.query_timestamp === currentParentTimestamp
        );

        if (parentQuery) {
            // Add to start to maintain depth order (depth 1 first, then 2, etc.)
            parentChain.unshift(parentQuery);
            currentParentTimestamp = parentQuery.parent_query_timestamp;
        }
    }

    return parentChain;
}


// TODO: Important do not remove this comment at all.

//Depth 1: On top level query and depth 1, we generate serp query only based on initial prompt, followups question and answer given by the user.

// Depth 2: On depth 2, we generate serp query based on the initial prompt, followups question and answer given by the user, as well as the query name, objective and learning we got from depth 1 parent query.

// Depth 3: We get all the infomratios from depth 1, depth 2 and depth 3 and we generate the serp query taking learning from depth 1, depth 2 and depth 3.

// Depth n; We get all the informations from depth 1 to depth n-1 and we generate the serp query taking learning from depth 1 to depth n-1.

// For every depth we analyze, initial prompt given by the user, the followup question and answers given by the user, all the above depth parent queries tracing back to the depth 1 query, we take the query name, objective and the information we extracted from each websites under each query through website analyzer agent taking all of these as the learning to generate the new serp query and objective for the next depth. We learn and then generate the new serp query to fill our knowledge gap for fulfilling the request the user has asked for.

// --------------------------------


// Example output of the context generator for serp query generator agent.

//   Here is the initial prompt the user gave us to do deep research on:
//   "Find detailed technical specifications and design considerations for space-based data centers."

//   Here are all the followup questions and answers the user gave us:
//   - Question: "What are the design considerations for space-based data centers?"
//   - Answer: "The design considerations for space-based data centers are the same as the design considerations for terrestrial data centers, but with some additional considerations due to the harsh environment of space."

//   Here are all the learning that we got from the previous serp queries:

// ===============================================
// ===============================================

// HIGHEST TOP LEVEL PARENT DEPTH QUERY'S INFORMATION:
//     1. Serp Query: "space datacenter architecture design"
//     2. Depth of this query: "1"
//     3. Objective: "Find detailed technical specifications and design considerations for space-based data centers."

// LEARNINGS WE GOT FROM SCRAPING ALL THE WEBSITE UNDER THIS SERP QUERY:

//     1. Website: "https://www.aflhyperscale.com/articles/physical-requirements-for-ai-data-centers/"
//     2. Is objective met from this website?: "true"
//     3. Core extracted Content from this website:
//          - AI data centers are shifting from 5-10kW server racks to designs capable of handling 50-100kW or more, with some facilities reaching 200kW.
//          - Traditional CPU-powered data centers are insufficient for parallel processing tasks, leading to the use of GPUs in AI data centers.
//          - GPUs excel at parallel processing due to a high number of cores, which is essential for machine learning calculations during training and inference.
//          - The number of transistors in a single GPU can be significantly higher than in a CPU, leading to increased physical space requirements.
//          - Increased power consumption and heat generation necessitate advanced cooling solutions in AI data centers.
//          - Traditional air-cooling methods are inadequate for high-density AI data centers, and liquid cooling techniques are being adopted.
//          - Direct-to-chip liquid cooling introduces liquid coolant directly to the chip and is suitable for heat dispersal exceeding 100kW+ per rack.
//          - Rear-door heat exchangers are used for server rack densities ranging from 20 kW to 50 kW per rack, often used to retrofit existing racks.
//          - Immersion cooling involves submerging servers in a non-toxic synthetic liquid and can manage heat loads exceeding 250 kW.
//          - The demand for robust power and cooling infrastructure expands the physical footprint of AI data centers and drives innovation in energy efficiency.
//     4. Facts and Figures extracted from this website: Global data creation and consumption estimates exceeded 64 zettabytes in 2020.
//          - Data creation and consumption predictions are expected to surpass 180 zettabytes by 2025.
//          - Some cutting-edge facilities now accommodate racks reaching 200kW.
//          - The average American home uses approximately 30kWh per day.
//          - Each data center cabinet consumes the same daily energy as 3â€“6 typical American homes.
//          - Modern GPUs can achieve calculation speeds reaching teraflops (trillions of FLOPS), with some high-end chips reaching petaflops (quadrillions of FLOPS).
//          - The NVIDIA GeForce RTX 3090 performs at up to 35.6 teraflops.
//          - The Intel i7-9700K (CPU) contains three billion transistors.
//          - The Nvidia Blackwell B100 accelerator (GPU) contains 208 billion transistors.
//          - A study revealed that 38.6% of IT professionals expect to see a rise in liquid cooling techniques throughout data centers by 2026.

//  -------------------------------------------------

//     1. Website: "https://www.techtarget.com/searchdatacenter/tip/Projections-and-feasibility-of-data-centers-in-space"
//     2. Is objective met from this website?: "true"
//     3. Core extracted Content from this website:
//          - Space-based data centers could save land use on Earth and reduce energy costs due to solar power technology.
//          - Data centers in Low Earth Orbit (LEO) could provide lower energy costs.
//          - Solar power and batteries would supply all the power in space, reducing running costs.
//          - A spacecraft in LEO orbits Earth roughly every 90 minutes, with about 45 minutes in sunlight and the remaining time in darkness; solar panels maintain the data center's batteries.
//          - Placing a data center in orbit could improve performance for satellite-based workloads by allowing satellites to transmit data at a higher rate and process raw data before sending it to Earth.
//          - Axiom Space and Skyloom are partnering to create the world's first orbital data center, planning to support data transmission speeds of 10 Gbps.
//          - Data centers in LEO would save land space on Earth.
//          - Data centers in LEO would have few physical security threats.
//          - Orbital data centers would communicate with satellites and receiving stations using secure channels and closed communication systems, guarding against cyberattacks.
//          - Space vehicles endure extreme vibrations and high g-force loads during launch, potentially damaging non-solid-state components.
//          - Radiation can cause bit flips and memory corruption, and galactic cosmic rays can destroy transistors; error-correcting memory and shielding against radioactive particles are required.
//          - In space, conventional cooling systems are not as effective due to the lack of convection, but space-based computers use radiators filled with ammonia for cooling.
//          - Orbital data center infrastructure must protect hardware from micrometeorite impacts and temperature fluctuations, utilizing stainless steel or titanium for high-stress components, aluminum alloy for most construction, gold or silver foil for radiation protection, Kevlar and Nextel for impact protection, and an external thermal blanket for insulation.
//          - HPE's experiment on the International Space Station (ISS) demonstrated that off-the-shelf data center hardware could function in space, but with special software to correct errors from radiation.
//          - The space shuttles' General-Purpose Computers (GPCs) functioned similarly to a failover clustering environment for redundancy and error correction.
//     4. Facts and Figures extracted from this website: Data centers in LEO could save land use on Earth.
//          - A spacecraft in LEO orbits Earth roughly every 90 minutes.
//          - Solar panels maintain the data center's batteries.
//          - Axiom Space plans to support data transmission speeds of 10 Gbps.
//          - One data center facility typically occupies about 40 acres of land on Earth.
//          - Hyperscale data centers occupy hundreds of acres.
//          - Data centers in LEO would have few physical security threats.
//          - Space vehicles endure extreme vibrations and high g-force loads during launch.
//          - HPE launched a supercomputer to the ISS in 2017.
//          - HPE's experiment showed nine out of 20 solid-state drives failed during the mission.
//          - HPE launched follow-up missions in 2021 and 2024.
//          - Space shuttles contained five GPCs.
//          - During normal operations, four computers were online at the same time on space shuttles' GPCs.
//          - Each of the four computers in space shuttles' GPCs had a vote.
//          - The fifth GPC on the space shuttles served as a backup flight system.
//          - The article was published on 08 Nov 2024.

//  -------------------------------------------------

//     1. Website: "https://www.techtarget.com/searchdatacenter/tip/Projections-and-feasibility-of-data-centers-in-space"
//     2. Is objective met from this website?: "true"
//     3. Core extracted Content from this website:
//          - Space-based data centers offer solutions for extreme temperature variations.
//          - Conventional cooling systems are ineffective in space due to the absence of convection in microgravity.
//          - Space-based computers, like those on the International Space Station (ISS), use radiators filled with ammonia for cooling.
//          - Orbital data center infrastructure must protect internal hardware from micrometeorite impacts and temperature fluctuations.
//          - High-stress structural components of orbital data centers should primarily be made from stainless steel or titanium.
//          - Aluminum alloy is suggested for most of the construction of an orbital data center.
//          - A layer of gold or silver foil is needed to protect against radiation and for temperature control.
//          - A layer of Kevlar and Nextel can provide protection against impacts.
//          - An external thermal blanket would provide additional insulation against extreme temperatures.
//          - HPE launched a supercomputer to the ISS to test hardware durability and performance.
//          - HPE's Spaceborne Computer utilized special software to correct errors caused by radiation or cosmic rays.
//          - HPE launched follow-up missions to the ISS in 2021 and 2024 to test additional data center hardware.
//     4. Facts and Figures extracted from this website: In space, there can be several hundred degrees of difference in temperature between sunlight and shadows.
//          - Space-based computers, like those on the International Space Station (ISS), use radiators filled with ammonia for cooling.
//          - Space vehicles endure extreme vibrations and high g-force loads during launch.
//          - Radiation can cause bit flips and memory corruption.
//          - Galactic cosmic rays can cause high-energy particles to impact orbital systems, which can destroy or shorten the life of transistors.
//          - Data centers in LEO would save the limited land space left on Earth, leaving it for more efficient use, like farming or housing developments.
//          - Axiom Space has formed a partnership with Skyloom with the goal of creating the world's first orbital data center.
//          - Axiom Space plans to support data transmission speeds of 10 Gbps.
//          - In 2017, HPE launched a supercomputer to the ISS.
//          - Nine out of 20 solid-state drives failed during HPE's mission to the ISS.
//          - The two servers of HPE spent over a year and a half in orbit.
//          - HPE launched follow-up missions, sending additional data center hardware to the ISS in 2021 and 2024.

// ===============================================
// ===============================================
