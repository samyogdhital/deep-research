// IMPORTANT: Do not remove the comment at all. And also do not remove the below schema at all ok?
// This is the full schema that we need to send to the frontend after we completed the deep research. This deep research should handle as many depth as well as breadth. And in the db, data must be stored progressively in the same schema as below. This is the final schema that will be in the db. Every step we take, we must iteratively and progressively store that data in the reuqired placed required place from the below schema in the db.
// Every step we do on the backend will be saved on the db not once but progressively, every step. And on the frontend as well through websockets. 


// This below is the exact schema of database. We can't change this even slighly. The logic and the rational of having each keys and values is also explaind below.
const final_database_schema = {
    // Make sure you iniitalize the report_id with a random uuid when the '/api/research/questions' route is called. We initalize the report_id there. And say for some reason some is directly called '/api/research/start' without calling '/api/research/questions' first, then also make sure you iniitalize the report_id '/api/research/start' as well. We need to check when anyone hits '/api/research/start'. If `report_id` is sent then we do db update and all for that report_id. If not present then we iniitalize the report_id there and then do db update. And since we are returing the whole db data for that report_id when the api resolves the user will know the report_id eventhough he/she forget to give it to the api initially.
    report_id: "asdf1234-asdfasdf123-asdfas123-asdf123", //"Unique identifyer for this whole research, this value will be set when we start the deep research only not before that.",
    initial_query: "User's initial detailed query that user gives us.",
    depth: 1, // This value must be between 1 - 10. User will give this value as well. This defines how much deep we have to go to in particular topic taking pervious learnings into considerations.
    breadth: 1, // This value also must be bewteen 1 - 10. User will give this value as well. This defines how much breadth do we cover. And prompt query generator agent will generate that much queries in a given depth.
    followUps_num: 5, // This value as must be in between 1 - 10.
    followUps_QnA: [
        // User will give detaild prompt in above initial_query, and also depth and breadth of the deep research. Then prompt analyzer agent after analyzing user's initial questions. Will generated `followUps_num` number of followup questions to better understand the user's current understanding as well as to understand the user itself so that query-generator agent can generate more target keywords that fulfills the user's deep research requirement.
        {
            id: 1, //  unique id of the question. This value will be sequential, first question will be  starting from 1 and last question will be `followUps_num`.
            question: "question generated by prompt analyzer agent after analyzing user's initial questions",
            answer: "User gives the answer to this question.",
        },
        // ... This can be as `followUps_num` of questions and answers prompt-analyzer agent will generate at once and user have to give the answer to each questions and submit once. 
    ],
    serpQueries: [
        {
            query: "Query that was searched on searxng and used to find websites",
            objective: "Very detailed objective of this serp query so that if we get websites, we scrape that and then website analyzer agent can only extract the information that meets this objective.",
            query_rank: 1, // At what order it was called? Was it the first or last serp query? It is alos the identifyer of this query.
            successful_scraped_websites: [
                {
                    url: "https://www.example.com",
                    title: "Title of the website which we get from searxng reponse.",
                    description: "Description of the website which we also get from searxng reponse.",
                    isRelevant: "Website analyzer agent will give absolute score from 1 to 10 based on the relevance of the content in the website to fully met the objective.",
                    extracted_from_website_analyzer_agent: [
                        "most relevent information that was extracted from the website that  met the above objective.",
                        "All the facts and figures and highly relevent information from the website in array of string fomrat.",
                        "...... can be as many but make sure that every information is highly relevant if not don't even include it."
                    ],

                }
            ],
            failedWebsites: ["array of string of websites that were failed to scrape and we also did not invoke website analyzer agent on them. They were also not included for information crunching and report writing."],
        }

    ],
    information_crunching_agent: {
        // We crunch information per serp query. We consider all the relevent websites only above 7 relevance score from website analyzer agent here. And from them we even crunch down the information even more and even decrease the unrelevance information here. Only include the websites that are even more relevant to the objective. And this will be used for report writing.
        serpQueries: [
            {
                query_rank: "Serp query's rank which is also the identifyer of that query.",
                crunched_information: [{
                    url: "https://www.example.com",
                    content: ["Array of string of content that was crunched by the information crunching agent."],
                },
                    // ... Can be as many objects as needed.
                ]

            }
        ]

    },
    report: {
        title: "Report title",
        sections: [
            {
                rank: "Section rank, at which rank show we show this section? This is also the identifyer of this particular section ok?",
                sectionHeading: "H1 or H2 or H3 but in markdown format with appropriate markdown tags ok? so that it's easy to parse markdown and show in frontend.",
                content: "Content in full markdown format. Include anything that you want list, url citations in [rank number of the website from citedUrls below.](id of url that is below there in citedUrls.) format. And everything thing ok?"
            },
            // .... this can be as many objects as needed.
        ]
        ,
        citedUrls: [
            {
                rank: 1, // Most highly cited url on the report must be 1 ok? And according to the rank have them sequentially. This is alos the unique identifyer of this url.
                url: "https://www.example.com",
                title: "Title of the website.",
                oneValueablePoint: "One valueable infomration, facts or figures that meets our objective and is used to write this report."

            }
        ]


    },

}