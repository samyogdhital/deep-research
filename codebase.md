### Detailed working of this whole codebase.
This whole code base is responsible for buildinga highly autonomous deep research agents.  the task of this agent is it will analyze the user's query precisely. The user will give the initial detailed prompt about the topic it want to research on, his current knowledge and what does he want to achive, or learn from this deep research agentic run. This information will be analyzed by master agent such that will generate some list of follow up questionsand user need to answer. User will answer this follow up Questions. After the follow-up questions are answered by the user then uh this agent(master agent) analyzes the users initial query as well as follow-up questions query And generate some initial set of serp query with certain specific set of objectives to precisely get the information that user is asking by scraping the websites. Under each serp query generated by the agent, the codebase will get list of websites where is high probability of finding the information the user is asking. So we will get the list of websites. Then the codebase will try to scrape all the list of websites. And on the successfully scraped list of websites, we will deploy a website analyzing agent. This agent will get the objective task from the master agent that generated the serp query. So now the website analyzer agent for each website will get the objective and the content scraped from the internet.

Website analyzing agent will:
- analyze the content from the website taking the objective into consideration.
- If the Information on the website precisely satisfies the objective and the query that user has askedthe objective and the query that user has asked, Then the precise content  that literally answers the user question will be extracted by the agent and the supporting quote or facts and figures that precisely supplement the objective of the query given by the master agent will be extracted in json format in a variable that tracks the list of all the website analyzing agent, their extracted content(json) and the website they scraped in.
- This precise and highly deterministic objective given to the website analyzer by the master agent will be same for all the websites we got under a specific serp qurey.
- If the websites was unsuccessful to scrape, we just leave it.
- This serp query to get the list of websites. Scrape that website. If successful every successfully scraped websites will have its own website scraping agent. "And this agent will actually extract the valuable infromation from the website.". Keep this in mind this is the actual agent that does the valuable information extraction part. So this process will happen in parallel for all initial set of serp query the master agent has generated. The extracted value will be kept and tracked in a variable for later detailed highly technical report by research writing agent.

### Deep Research
So all the initial set of queries will complete. And then the code will see the breadth and width input. If the breadth is more than one then another set of serp query but taking learnings from previous whole serp query will be initiated. Then the same process of serp query, get list of websites, scrape website, successful website will trigger website analyzing agent. Agent will scrape the actual valuable data according to the objective give by master agent.

### Depth vs Breadth impact in research process.
User initially also gives "breadth" and "depth" in the research process that user wants this agent and the whole codebase to go. This controls how many independent queries you start with and how many rounds of recursive research you do.

Breadth:
When you set a higher breadth (e.g. 2 vs. 1), the master agent generates more SERP queries.

Breadth 1: Only one query is generated, so you get one list of websites.
Breadth 2: Two queries are generated, meaning you explore two different “angles” of your question. This gives you more varied starting websites and can increase the chance of finding additional relevant information.
Depth:
Depth controls how many rounds of follow-up research you do.

Depth 1: The process stops after the initial SERP query, scraping, and analysis.
Depth 2: After gathering and analyzing initial results, the system uses the learnings (the extracted content and insights) to generate a new set of queries and redo the scraping and analysis. This recursion builds on the previous findings, digging deeper into the topic.

So the recursive flow is The master agent analyzes the user’s question and generates an initial SERP query (or multiple, if breadth > 1).
For each query, the system scrapes websites and extracts valuable, objective-specific content.
If depth equals 1, the process ends here.
If depth is greater than 1 (e.g. 2), the system adds a recursive stage where it uses the current findings to generate new queries and further research, expanding and deepening the investigation. This is how the deep research is happening in this codebase.

### Token count and report writing and extracting the valuable information.
One main problem here in the codebase is, if you do alot of serp query and scrape alot of data on from the internet your question also requires that, it is not surprising you very very quickly pass the context limit of one of the highest llm in the world i.e. 2M tokens. And not only that, the model's effective performance significantly decrease after 50k tokens.
To solve that problem, what we do is we constantly track the token count after we scrape each website. If the token count is reaching or just exceeds 50k token, we spin up a new Information Crunching Agent.

### Query Generating Agent
Requirement Summary:

Parallelization in Deep Research (@deep-research.ts):

Top-level (depth 1) serp queries should run in parallel.
Each top-level query proceeds to its next depth as soon as it finishes—its subsequent depth queries must execute independently without waiting for sibling queries from other top-level nodes.
For example, if one top-level query completes depth 1 faster than others, immediately generate and execute its depth 2 queries without synchronizing with the others.
Contextual Query Generation Update:

Update how generateQueriesWithObjectives is called in @deep-research.ts to incorporate the new function generateContextforSerpQueryGeneratorAgent from @query-generator.ts.
When generating a new serp query (e.g., at depth 5), pass an array of all previous queries (from depth parent depth to the depth 4 by tracing "parent_query_timestamp") in the correct order. This array should use the type SerpQueries[] as defined in the original DB schema from @index.ts without any modification.
Additionally, pass along initial_user_prompt and followUps_QnA to provide full context for generating the next query.

### Information Crunching Agent
The task of this agent is code will spin this agent up after every 50k token give or take we scrapped from the websites we got from serp query generated by the master agent.
- Code will count the token we reach every time we scrape a website under a specific serp query and objective. And this token tracking will be different for different serp query.
- Websites under same serp query will get scrapped and each successfull website scrapped, we track the token we scrapped. If the overall scrapped content is reaching 50k tokens, regardless of number of websites scrapped under specific serp query, we spin up a Information Crunching Agent and this agnet will take the detailed objective given by master agent for that specific serp query. See what we are doing here is, we are doing here is we are scraping more and more websites and we are crunching more and more the most valueable information we can get from the internet by scraping the websites with specific agenda given by the master agent. So we crunch as much valuable, value pack and highly factual and figures, highly related technical detail within few tokens as possible with most concise value packed information from too many websites so that we can reach near to the aboslute solution to the problem or answer to the question the user is asking.
- See a website will not be very efficient with words and information we are here being too much efficient with tokens as well as are obvessed with the most related, and highly to the point, 100% to the question that we scrape from the internet. We share it to the master agent in th next iteration of serp query generation whether it is through depth or breadth. And so as we scrape more and more related websites and only extract the abosolute needed and best information as possible, we then pass it to the report writing agent.

### Report writing agent
The task of this report writing agent is to collect all the facts and incremental learnings we have not the random bluff we get from every website but only the cream information we get from website analyzer agent and this Information Crunching Agent. And then write the report such that the report will in very very highly long with every word being pure value to the user technically detailed research paper or report to the user such that the answer of user is 100% answered. Here the report writing agnet must not use his memory but use the soure and data it got to write report. In every section of the report, every sentence that you make must be cited from the highly credible website and source, with precise word or sentence or facts or figures extracted from there. And also at the last of the report we have all the list of websites that were considered to write the report and by neglecting all the bluff source and websites.
- If the depth and breath is small and we are under the absolute token limit of a report writing agentt that is 200k words then that is not a problem, Information Crunching Agent will go and crunch the information and get the most cream infomration from these list of website nalyzing agents that themselves extract only cream result from highly relevent websites. And there information are then collected by the Information Crunching Agent and then even crunches them for report writing. Crunching here mean shear amount of value we compress within certain length of sentence or paragraph without missing any important and highyl relevent related information shared and extractd by webstie analyzer agents. So, website analyzer agents will extract alot of valuable infromation from websites under specific agenda. And  Information Crunching Agent even cruches that but also tracks the information for citations, liek wehre that information was extracted from and which website and what particular quote, facts figures and all.
- If the research is soo deep and breadth that the context window of report writing agent is alos exceedin that is 200k words after even counting all words and tokens of all the information of Information Crunching Agent, we will not at this state then stop the research and write the very detailed technical report.


### Error handling 
By any chance the code get's the critical error in the middle of the research like code not working at all or the report generation is not working at all and all these things, but remember if any website is not successfully scraped that should be considered critical error. So at that time, if you get any critical error, a file named error-output.md file must have all the infomration we have extractd and all the websites we scrapped that were successful and websites that were not successful and alos all the precise crunched infromation from website analyzing agent for individual agent and if Information Crunching Agent was used and called, the precise answer from that everything needs to be strucutrally be in that error-output.md file.

### Check for the whole implementation of above architecture
Everytime you take reference to this file, check and verify if your code changes 100% implements the feature discussed in codebase.md file? and if it is absolutely necessary then only make the changes. But make sure we are 100% implementing the exact flow of the codebase and agent according to codebase.md file.

### Key Consideration
- Make sure every agent(LLM) call we are doing it must have detailed schema file and important filed must be listed as required on that schema. You will get response in string format, you need to parse that string to convert it to json format and then access the desired key from that parsed object.
- Citatios are everything in the whole agentic workflow. Whatever information we extract, we never ever not include citations and source of that information wheteher it is passing from website analyzer agent to Information Crunching Agent or Information Crunching Agent to report writing agent or may be Information Crunching Agent agent to query generating master agent.


# Detailed Deterministic Requirements for Deep Research Codebase

This document outlines the precise requirements and process flows for the entire deep research codebase. It incorporates the original architecture along with the new detailed requirements regarding the handling of large-scale web scraping, data extraction, and parallel information crunching while maintaining citation integrity and schema consistency.

---

## 1. Core Architectural Flow

- **Master Agent Responsibilities:**
  - Analyze the user’s initial detailed query.
  - Generate a set of SERP queries (number depends on user-specified breadth and recursive depth).
  - Provide specific objectives for subsequent agents (Website Analyzing Agents, Information Crunching Agents) based on the query.
  - Ensure all downstream agents strictly adhere to the generated objective.

- **SERP Query Process:**
  - For each SERP query, retrieve up to 7 top URLs.
  - Initiate website scraping for each URL.

- **Website Scraping and Analysis:**
  - Each successfully scraped website triggers a Website Analyzing Agent.
  - Each Website Analyzing Agent extracts the most valuable and relevant information that directly satisfies the specific objective provided by the Master Agent.
  - Irrelevant or non-useful scraped data is discarded.
  - The extracted information is stored along with precise source citation details.

---

## 2. Token/Word Tracking and Threshold Management

- **Real-Time Content Tracking:**
  - As soon as the first Website Analyzing Agent returns its output, initiate a real-time counter to track the cumulative word/token count per SERP query.
  - Continuously update the counter with each subsequent successful extraction.

- **Threshold-Based Triggering:**
  - **Primary Threshold:** If the total content from Website Analyzing Agents remains below ~300K words (or tokens), the system can proceed without invoking further processing.
  - **Exceeding Threshold:** Once the accumulated content exceeds ~300K words (or reaches a higher threshold such as 500K words in extreme cases), the system must invoke the Information Crunching Agent for that specific SERP query.

---

## 3. Information Crunching Agent (ICA)

- **Invocation Conditions:**
  - Trigger the ICA separately and in parallel for each SERP query when its associated content crosses the threshold.
  - The ICA should be invoked per query; if multiple queries exist, multiple parallel instances of the ICA will run concurrently.

- **Operational Details:**
  - **Chunking:** The ICA processes content in chunks (approximately 50K words per call). If one call cannot compress all the data, additional ICA calls are invoked sequentially or in parallel until all content is processed.
  - **Data Compression:** The ICA’s purpose is to condense the raw extracted data while ensuring:
    - The retention of critical, highly relevant information.
    - No loss or obfuscation of essential details such as factual figures, quotes, and direct citations.
  - **Citation Integrity:** Each piece of condensed information must maintain a direct reference to its original source. This includes preserving the website URL and the specific quote or fact.

- **Final Output for Report Writing:**
  - The output from the ICA should be concise and must not exceed the final report’s context window (maximum ~300K words).
  - The processed output is then passed to the Report Writing Agent.

---

## 4. Report Writing Agent

- **Data Ingestion:**
  - Receives aggregated and condensed data from:
    - Website Analyzing Agents (if overall content remains under threshold).
    - Information Crunching Agents (if content exceeds the acceptable limit).
  - Ensures that the final payload for report writing is within the model’s context window.

- **Report Generation:**
  - The report is constructed using only the condensed, highly relevant data.
  - Every section and sentence in the final report must include explicit citations from the original sources.
  - The report format must remain consistent with the front-end specifications and pre-defined schema.

---

## 5. Handling High Breadth and Depth Scenarios

- **Recursive Query Expansion:**
  - For higher breadth, the Master Agent generates additional SERP queries exploring multiple angles of the original query.
  - For deeper research (recursive depth > 1), follow-up queries are generated based on the learnings from previous rounds.
  - The same token tracking, website analysis, and information crunching processes apply at each recursive level.

- **Parallel Processing Across Queries:**
  - Each SERP query is processed independently, with its own set of Website Analyzing Agents and, if necessary, its own parallel ICA instances.
  - The cumulative information from each SERP query is maintained separately to ensure that the data does not exceed per-query thresholds.

---

## 6. API Calls and Schema Requirements

- **Definite Schema for LLM Calls:**
  - Every API call must include a well-defined, type-safe schema. Use the Google Generative package or the current schema definitions as a reference.
  - LLM responses are expected as stringified JSON and must be parsed into JSON objects for further processing.
  - Any modifications to the schema must be absolutely necessary and must not disrupt the existing workflow.

- **Schema Integrity:**
  - The schema must include fields for:
    - Extracted content.
    - Source citation details (website URL, quotes, factual data).
    - Objective details provided by the Master Agent.
  - Ensure that every layer (Website Analyzing, Information Crunching, Report Writing) retains the source and objective metadata.

---

## 7. Error Handling and Logging

- **Critical Error Protocol:**
  - If a critical error occurs (e.g., failure to scrape a website or failure in the report generation process), an `error-output.md` file must be generated.
  - The error file should contain:
    - Detailed logs of all successful and failed website scrapes.
    - The extracted content and citations from each Website Analyzing Agent.
    - The outputs from any invoked Information Crunching Agents.
  - This ensures traceability and aids in debugging.

---

## 8. Implementation Guidelines

- **Maintain Existing Workflow:**
  - All new features must be integrated without breaking the existing workflow.
  - The new requirements should seamlessly extend the deep research process while preserving the core architecture.

- **Parallel and Asynchronous Processing:**
  - Leverage asynchronous or multi-threaded processing to invoke Information Crunching Agents in parallel for each SERP query.
  - Ensure that token tracking is efficient and real-time to avoid delays or data overflow.

- **Data Preservation and Citation:**
  - It is imperative that every extracted piece of information retains its source citation.
  - The design must prevent any loss of source data during any transformation or aggregation process.

---

By adhering to these detailed deterministic requirements, the deep research codebase will efficiently handle large volumes of data while ensuring that the final report is concise, accurate, and fully traceable back to its sources.


### High Level Flow
```
    A[Initial Query] --> B[Generate 5 SERP Queries Level 1]
    B --> C[Process Each Query]
    C --> D[Recursive Depth with Math.ceil(breadth/2)]
    D --> E[Level 2: 3 Queries]
    E --> F[Level 3: 2 Queries]
    F --> G[Level 4: 1 Query]
    G --> H[Level 5: 1 Query]
```

### Detailed Query Flow
- Depth Level 1 (Initial)

    - Breadth = 5
    - Generates 5 SERP queries
    - For each query:
        - Get search results (up to 7 URLs per query)
        - Scrape websites
        - Analyze content
        - Token tracking & information crunching if needed

- Depth Level 2

    - Breadth = Math.ceil(5/2) = 3
    - Generates 3 new queries based on findings
    - Process same as level 1
    - Total queries so far: 5 + (5×3) = 20

- Depth Level 3

    - Breadth = Math.ceil(3/2) = 2
    - Generates 2 queries per previous query
    - Total queries so far: 20 + (15×2) = 50

- Depth Level 4

    - Breadth = Math.ceil(2/2) = 1
    - Generates 1 query per previous query
    - Total queries so far: 50 + (30×1) = 80

- Depth Level 5

    - Breadth = 1
    - Final deep dive with 1 query per previous query
    - Total queries so far: 80 + 30 = 110

### Per Query Process
for each query:
  1. Generate SERP results (gets max 7 top URLs)
  2. Scrape websites
  3. For each successful scrape:
     - Analyze content against objective
     - Track tokens
     - If tokens > 50k:
       - Trigger information crunching
     - Store results
  4. After all websites processed:
     - Final information crunching if needed
  5. Start recursive depth if depth > 1

### Image representation of the whole process.
  ![alt text](image.png)

### 3 absolute requirement that you should should not miss.
Since this is our minimal viable product implementation, We don't want to make it overly complex. We only have 3 requirement.
- The deep research codebase must handle as many deep(depth parameter) as well as broad(breadth parameter) research.
- There is a clear defined schema in `response-schema.ts` file as well as detailed comment on top. Whatever we do the data need to be save progressively. If something need to be inside that db and is absolutely required by db schema, we right there and then save that on db on our code `deep-research.ts` file or `server.ts` file. Progressive db update. Every step is saved on db following the fixed db schema which we cannot miss at all.
- And final requirement is whatever update we do on the backend side, we will send that update right there and then to the frontend client so that they can react on that on frontend through websocket connection.
Make sure these rules are not broken at all following our MVP philosophy. Let's not make this codebase overly complex. If there is some redundant logic that does not support these absolute requirement, we remove that from codebase ok?

### Websocket logic
![Websocket logic](websocket-logic.png)



### Realtime websocket events that needs to fired in each process of during deep research.
Here are all the processes and their corresponding events with exact keyword that need to be fired:
(Make sure you understand the semantic meaning of each event name and where they should be fired. And alos make sure the number of events is exactly the same as below. And the individual event name is alos precisely the same as listed below.)
1. Research Start Process
    Events:
      - generating_followups
      - followups_generated
2. Information Gathering Process
    Events:
      - new_serp_query
      - got_websites_from_serp_query
      - scraping_a_website
      - analyzing_a_website
      - analyzed_a_website 
3. Information Crunching Process (triggered when words > 50k)
    Events:
      - crunching_a_serp_query
      - crunched_a_serp_query
4. Report Writing Process
    Events:
      - report_writing_start
      - report_writing_successfull

### Few details while implementing above websocket on backend as well frontend.
- Make sure that before firing any event through the websocket, we first save the recent event that triggered this websocket event to the database. (Like it may be, we have got the list of websites after hitting searxng api with serp query generated by query generator agent. First you save the list of websites on databse and after that it is saved to the database, then only you fire websocket event for that event.) Then we fire the websocket event for that event, and with that websocket payload, we are sending the database update(literally all the data we have stored in the database up until firing this websocket event for that particular research_id.) through the websocket event. And we do this for every event we fire from the backend. We update the data on the database, we wait for the data to be stored in the database, then we fire the websocket event and along with that we get the latest data from the db, right after we saved the event to the database before firing this event. We get that and send that db data as payload along with the websocket.
- While firing "got_websites_from_serp_query", "scraping_a_website", "analyzing_a_website", "analyzed_a_website" these events, first when we get the new serp query from query-generator agent, we fire a single event called "new_serp_query". Then after that we hit that query to the searxng api. Now we get the list of websites from searxng webites, we fire another event "got_websites_from_serp_query". Then understand this, say we got "X" number of websites under that query. when we get the list of websites from a serp query, we fire "X" numbers of "scraping_a_website" event. As these individual websites are successfully scraped, we fire "X" number of "analyzing_a_website" event. Then after the response from website analyzer website is successful, we fire, "analyzed_a_website" for each of these websites. So "scraping_a_website", "analyzing_a_website", "analyzed_a_website" these 3 events are events fired for individual websites ok? Keep that in mind. And if you go inside @response-schema.ts file inside deep-research/src/db folder, With each successful step, the status of individual website object inside "successful_scraped_websites" key inside db, changes from "scraping" to "analyzing" and finally "analyzed". Please read that file to understand this concept in depth.
- And for information crunching, its straight forward, we fire an event before information crunching per query, and fire another event after information crunching was successful. Save everything to db before firing event.
- On frontend nextjs side, backend will fire, "generating_followups" and "followups_generated" Research Start Process, we may catch that on frontend but we don't need to show that on the frontend side. On frontend the actual ui of realtime event will start after deep research process is started, after the user submits the answer of followup questions and the deep research is literally started.

### Websocket logic and implemenation on frontend side.
 Detailed Front-End Requirements for Real-Time Research Logs:
 1. Research Persistence Across Browser Tabs:
    - Research processes can last from 1 to 2 hours.
    - If the initiating browser tab is closed, the next time a user opens a tab, the UI must display:
         • The complete historical log of events that have already occurred.
         • The real-time logs continuing from where they left off.

 2. Sidebar Navigation Structure:
    - The sidebar should include:
         • Standard report sections for past research (e.g., "Today", "Previous 7 Days", "Previous 30 Days").
         • An additional "Ongoing Research" section dedicated solely to research processes currently in progress.
    - For ongoing research processes:
         • Display a skeleton loader for each active research.
         • Each skeleton loader should match the exact width and height of the report title element.

 3. Handling Ongoing Research UI Interaction:
    - In any browser tab (even those that did not initiate the research):
         • The "Ongoing Research" section must visibly show the appropriate number of skeleton loaders corresponding to the number of active research processes.
    - When a user clicks on an individual skeleton loader:
         • The UI should immediately display the real-time logs for the selected research.
         • These logs must include all events from query generation, website scraping, information crunching, data extraction, and report writing.

 4. Websocket Events and Data Synchronization:
    - Each time an event occurs on the backend:
         • The event details are first saved to the database.
         • After successful database update, a websocket event is fired with the complete snapshot of the current database state.
    - The payload of each websocket event adheres strictly to the fixed database schema defined in response-schema.ts:
         • The schema remains constant across all events from research initiation to report writing.
         • Although the data values update as new events occur, the structure (keys and types) is always consistent.
    - The front-end must use this snapshot to update the UI in real time.

 5. Isolation of Research Logs:
    - When switching between different ongoing research processes (by clicking on different skeleton loaders):
         • The UI must cleanly update to show only the logs of the selected research.
         • Logs from different research processes should never mix or collide.

 Summary:
 The front-end implementation must:
    - Persist and restore full research logs across browser sessions.
    - Clearly segregate past reports and ongoing research in the sidebar.
    - Dynamically update real-time logs based on websocket events that deliver a database snapshot.
    - Ensure that selecting a specific ongoing research item shows its corresponding log history without interference from others.