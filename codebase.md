### Detailed working of this whole codebase.
This whole code base is responsible for buildinga highly autonomous deep research agents.  the task of this agent is it will analyze the user's query precisely. The user will give the initial detailed prompt about the topic it want to research on, his current knowledge and what does he want to achive, or learn from this deep research agentic run. This information will be analyzed by master agent such that will generate some list of follow up questionsand user need to answer. User will answer this follow up Questions. After the follow-up questions are answered by the user then uh this agent(master agent) analyzes the users initial query as well as follow-up questions query And generate some initial set of serp query with certain specific set of objectives to precisely get the information that user is asking by scraping the websites. Under each serp query generated by the agent, the codebase will get list of websites where is high probability of finding the information the user is asking. So we will get the list of websites. Then the codebase will try to scrape all the list of websites. And on the successfully scraped list of websites, we will deploy a website analyzing agent. This agent will get the objective task from the master agent that generated the serp query. So now the website analyzer agent for each website will get the objective and the content scraped from the internet.

Website analyzing agent will:
- analyze the content from the website taking the objective into consideration.
- If the Information on the website precisely satisfies the objective and the query that user has askedthe objective and the query that user has asked, Then the precise content  that literally answers the user question will be extracted by the agent and the supporting quote or facts and figures that precisely supplement the objective of the query given by the master agent will be extracted in json format in a variable that tracks the list of all the website analyzing agent, their extracted content(json) and the website they scraped in.
- This precise and highly deterministic objective given to the website analyzer by the master agent will be same for all the websites we got under a specific serp qurey.
- If the websites was unsuccessful to scrape, we just leave it.
- This serp query to get the list of websites. Scrape that website. If successful every successfully scraped websites will have its own website scraping agent. "And this agent will actually extract the valuable infromation from the website.". Keep this in mind this is the actual agent that does the valuable information extraction part. So this process will happen in parallel for all initial set of serp query the master agent has generated. The extracted value will be kept and tracked in a variable for later detailed highly technical report by research writing agent.

### Deep Research
So all the initial set of queries will complete. And then the code will see the breadth and width input. If the breadth is more than one then another set of serp query but taking learnings from previous whole serp query will be initiated. Then the same process of serp query, get list of websites, scrape website, successful website will trigger website analyzing agent. Agent will scrape the actual valuable data according to the objective give by master agent.

### Depth vs Breadth impact in research process.
User initially also gives "breadth" and "depth" in the research process that user wants this agent and the whole codebase to go. This controls how many independent queries you start with and how many rounds of recursive research you do.

Breadth:
When you set a higher breadth (e.g. 2 vs. 1), the master agent generates more SERP queries.

Breadth 1: Only one query is generated, so you get one list of websites.
Breadth 2: Two queries are generated, meaning you explore two different “angles” of your question. This gives you more varied starting websites and can increase the chance of finding additional relevant information.
Depth:
Depth controls how many rounds of follow-up research you do.

Depth 1: The process stops after the initial SERP query, scraping, and analysis.
Depth 2: After gathering and analyzing initial results, the system uses the learnings (the extracted content and insights) to generate a new set of queries and redo the scraping and analysis. This recursion builds on the previous findings, digging deeper into the topic.

So the recursive flow is The master agent analyzes the user’s question and generates an initial SERP query (or multiple, if breadth > 1).
For each query, the system scrapes websites and extracts valuable, objective-specific content.
If depth equals 1, the process ends here.
If depth is greater than 1 (e.g. 2), the system adds a recursive stage where it uses the current findings to generate new queries and further research, expanding and deepening the investigation. This is how the deep research is happening in this codebase.

### Token count and report writing and extracting the valuable information.
One main problem here in the codebase is, if you do alot of serp query and scrape alot of data on from the internet your question also requires that, it is not surprising you very very quickly pass the context limit of one of the highest llm in the world i.e. 2M tokens. And not only that, the model's effective performance significantly decrease after 50k tokens.
To solve that problem, what we do is we constantly track the token count after we scrape each website. If the token count is reaching or just exceeds 50k token, we spin up a new Information Crunching Agent.

### Information Crunching Agent
The task of this agent is code will spin this agent up after every 50k token give or take we scrapped from the websites we got from serp query generated by the master agent.
- Code will count the token we reach every time we scrape a website under a specific serp query and objective. And this token tracking will be different for different serp query.
- Websites under same serp query will get scrapped and each successfull website scrapped, we track the token we scrapped. If the overall scrapped content is reaching 50k tokens, regardless of number of websites scrapped under specific serp query, we spin up a Information Crunching Agent and this agnet will take the detailed objective given master agent for that specific 