### Detailed working of this whole codebase.
This whole code base is responsible for buildinga highly autonomous deep research agents.  the task of this agent is it will analyze the user's query precisely. The user will give the initial detailed prompt about the topic it want to research on, his current knowledge and what does he want to achive, or learn from this deep research agentic run. This information will be analyzed by master agent such that will generate some list of follow up questionsand user need to answer. User will answer this follow up Questions. After the follow-up questions are answered by the user then uh this agent(master agent) analyzes the users initial query as well as follow-up questions query And generate some initial set of serp query with certain specific set of objectives to precisely get the information that user is asking by scraping the websites. Under each serp query generated by the agent, the codebase will get list of websites where is high probability of finding the information the user is asking. So we will get the list of websites. Then the codebase will try to scrape all the list of websites. And on the successfully scraped list of websites, we will deploy a website analyzing agent. This agent will get the objective task from the master agent that generated the serp query. So now the website analyzer agent for each website will get the objective and the content scraped from the internet.

Website analyzing agent will:
- analyze the content from the website taking the objective into consideration.
- If the Information on the website precisely satisfies the objective and the query that user has askedthe objective and the query that user has asked, Then the precise content  that literally answers the user question will be extracted by the agent and the supporting quote or facts and figures that precisely supplement the objective of the query given by the master agent will be extracted in json format in a variable that tracks the list of all the website analyzing agent, their extracted content(json) and the website they scraped in.
- This precise and highly deterministic objective given to the website analyzer by the master agent will be same for all the websites we got under a specific serp qurey.
- If the websites was unsuccessful to scrape, we just leave it.
- This serp query to get the list of websites. Scrape that website. If successful every successfully scraped websites will have its own website scraping agent. "And this agent will actually extract the valuable infromation from the website.". Keep this in mind this is the actual agent that does the valuable information extraction part. So this process will happen in parallel for all initial set of serp query the master agent has generated. The extracted value will be kept and tracked in a variable for later detailed highly technical report by research writing agent.

### Deep Research
So all the initial set of queries will complete. And then the code will see the breadth and width input. If the breadth is more than one then another set of serp query but taking learnings from previous whole serp query will be initiated. Then the same process of serp query, get list of websites, scrape website, successful website will trigger website analyzing agent. Agent will scrape the actual valuable data according to the objective give by master agent.

### Depth vs Breadth impact in research process.
User initially also gives "breadth" and "depth" in the research process that user wants this agent and the whole codebase to go. This controls how many independent queries you start with and how many rounds of recursive research you do.

Breadth:
When you set a higher breadth (e.g. 2 vs. 1), the master agent generates more SERP queries.

Breadth 1: Only one query is generated, so you get one list of websites.
Breadth 2: Two queries are generated, meaning you explore two different “angles” of your question. This gives you more varied starting websites and can increase the chance of finding additional relevant information.
Depth:
Depth controls how many rounds of follow-up research you do.

Depth 1: The process stops after the initial SERP query, scraping, and analysis.
Depth 2: After gathering and analyzing initial results, the system uses the learnings (the extracted content and insights) to generate a new set of queries and redo the scraping and analysis. This recursion builds on the previous findings, digging deeper into the topic.

So the recursive flow is The master agent analyzes the user’s question and generates an initial SERP query (or multiple, if breadth > 1).
For each query, the system scrapes websites and extracts valuable, objective-specific content.
If depth equals 1, the process ends here.
If depth is greater than 1 (e.g. 2), the system adds a recursive stage where it uses the current findings to generate new queries and further research, expanding and deepening the investigation. This is how the deep research is happening in this codebase.

### Token count and report writing and extracting the valuable information.
One main problem here in the codebase is, if you do alot of serp query and scrape alot of data on from the internet your question also requires that, it is not surprising you very very quickly pass the context limit of one of the highest llm in the world i.e. 2M tokens. And not only that, the model's effective performance significantly decrease after 50k tokens.
To solve that problem, what we do is we constantly track the token count after we scrape each website. If the token count is reaching or just exceeds 50k token, we spin up a new Information Crunching Agent.

### Information Crunching Agent
The task of this agent is code will spin this agent up after every 50k token give or take we scrapped from the websites we got from serp query generated by the master agent.
- Code will count the token we reach every time we scrape a website under a specific serp query and objective. And this token tracking will be different for different serp query.
- Websites under same serp query will get scrapped and each successfull website scrapped, we track the token we scrapped. If the overall scrapped content is reaching 50k tokens, regardless of number of websites scrapped under specific serp query, we spin up a Information Crunching Agent and this agnet will take the detailed objective given by master agent for that specific serp query. See what we are doing here is, we are doing here is we are scraping more and more websites and we are crunching more and more the most valueable information we can get from the internet by scraping the websites with specific agenda given by the master agent. So we crunch as much valuable, value pack and highly factual and figures, highly related technical detail within few tokens as possible with most concise value packed information from too many websites so that we can reach near to the aboslute solution to the problem or answer to the question the user is asking.
- See a website will not be very efficient with words and information we are here being too much efficient with tokens as well as are obvessed with the most related, and highly to the point, 100% to the question that we scrape from the internet. We share it to the master agent in th next iteration of serp query generation whether it is through depth or breadth. And so as we scrape more and more related websites and only extract the abosolute needed and best information as possible, we then pass it to the report writing agent.

### Report writing agent
The task of this report writing agent is to collect all the facts and incremental learnings we have not the random bluff we get from every website but only the cream information we get from website analyzer agent and this Information Crunching Agent. And then write the report such that the report will in very very highly long with every word being pure value to the user technically detailed research paper or report to the user such that the answer of user is 100% answered. Here the report writing agnet must not use his memory but use the soure and data it got to write report. In every section of the report, every sentence that you make must be cited from the highly credible website and source, with precise word or sentence or facts or figures extracted from there. And also at the last of the report we have all the list of websites that were considered to write the report and by neglecting all the bluff source and websites.
- If the depth and breath is small and we are under the absolute token limit of a report writing agentt that is 200k words then that is not a problem, Information Crunching Agent will go and crunch the information and get the most cream infomration from these list of website nalyzing agents that themselves extract only cream result from highly relevent websites. And there information are then collected by the Information Crunching Agent and then even crunches them for report writing. Crunching here mean shear amount of value we compress within certain length of sentence or paragraph without missing any important and highyl relevent related information shared and extractd by webstie analyzer agents. So, website analyzer agents will extract alot of valuable infromation from websites under specific agenda. And  Information Crunching Agent even cruches that but also tracks the information for citations, liek wehre that information was extracted from and which website and what particular quote, facts figures and all.
- If the research is soo deep and breadth that the context window of report writing agent is alos exceedin that is 200k words after even counting all words and tokens of all the information of Information Crunching Agent, we will not at this state then stop the research and write the very detailed technical report.