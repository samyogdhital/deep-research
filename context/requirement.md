Section 1: User Input and Follow-up Questions via /api/research/questions
When a user begins a research project, they first interact with the system by providing an initial prompt and a specific integer that indicates how many follow-up questions they want the system to generate. The initial prompt is a detailed string that describes the research topic. For example, a user might submit an initial prompt like "I want to research the latest advancements in renewable energy, focusing on solar and wind power" along with an integer value of 3, meaning they want the system to generate exactly three follow-up questions.
This data—the initial prompt and the integer for the number of follow-up questions—is sent to the backend through an HTTP POST request to the /api/research/questions endpoint. The request body is structured as a JSON object. For instance, it might look like this:
json

{
  "initial_prompt": "I want to research the latest advancements in renewable energy, focusing on solar and wind power",
  "num_questions": 3
}

Upon receiving this request, the backend validates that the initial prompt is a non-empty string and that the num_questions value is a positive integer greater than zero. If either validation fails, the system returns an HTTP 400 Bad Request response with an error message detailing the issue, such as "Initial prompt cannot be empty" or "Number of questions must be a positive integer."
Assuming the input is valid, the backend activates an AI component called the Prompt Analyzer Agent. This agent is responsible for processing the initial prompt and generating the exact number of follow-up questions specified by the user. The Prompt Analyzer Agent analyzes the initial prompt by breaking it down into its key concepts and identifying areas where additional clarification could refine the research scope. It uses natural language processing techniques to understand the intent and context of the prompt, then formulates questions that address potential ambiguities or gaps in specificity.
For the example prompt "I want to research the latest advancements in renewable energy, focusing on solar and wind power" with num_questions set to 3, the Prompt Analyzer Agent might generate the following questions:
"What specific aspects of solar power are you most interested in, such as efficiency, cost, or materials?"

"Are you looking for data on recent innovations or predictions about future trends in renewable energy?"

"Should the research prioritize onshore wind power systems, offshore wind power systems, or both?"

These questions are not generic; they are tailored to the prompt’s content and designed to elicit responses that will sharpen the focus of the research. The agent ensures that the number of questions matches the user’s num_questions value exactly—in this case, three questions. The questions are generated as a list of strings, and the backend returns them to the user in an HTTP 200 OK response with a JSON body like this:
json

{
  "research_id": "abc123",
  "followup_questions": [
    "What specific aspects of solar power are you most interested in, such as efficiency, cost, or materials?",
    "Are you looking for data on recent innovations or predictions about future trends in renewable energy?",
    "Should the research prioritize onshore wind power systems, offshore wind power systems, or both?"
  ]
}

Here, research_id is a unique identifier (e.g., a UUID) generated by the backend to track this specific research project throughout its lifecycle. The research_id is stored in the database along with the initial prompt and the generated follow-up questions, ensuring that all subsequent steps can be linked back to this initial request.
The user then reviews these follow-up questions and provides answers to each one. These answers, along with additional parameters for depth and breadth, will be submitted in the next step via the /api/research/start endpoint, which initiates the deep research process.
Section 2: Initiating Deep Research via /api/research/start
After receiving the follow-up questions, the user answers each one and decides on two additional parameters: depth and breadth. Depth is an integer that specifies how many recursive levels of research the system should perform, while breadth is an integer that determines how many initial search queries the system will generate at the top level. For example, based on the earlier questions, the user might provide answers like this:
For "What specific aspects of solar power are you most interested in, such as efficiency, cost, or materials?": "I’m interested in solar panel efficiency."

For "Are you looking for data on recent innovations or predictions about future trends in renewable energy?": "I want data on recent innovations."

For "Should the research prioritize onshore wind power systems, offshore wind power systems, or both?": "Focus on offshore wind power systems."

The user might then set depth to 5 and breadth to 5, meaning the system will perform five levels of recursive research starting with five initial queries.
This complete dataset—the initial prompt, the follow-up questions and their answers, depth, and breadth—is sent to the backend via an HTTP POST request to the /api/research/start endpoint. The request body is a JSON object structured as follows:
json

{
  "research_id": "abc123",
  "initial_prompt": "I want to research the latest advancements in renewable energy, focusing on solar and wind power",
  "followup_questions": [
    "What specific aspects of solar power are you most interested in, such as efficiency, cost, or materials?",
    "Are you looking for data on recent innovations or predictions about future trends in renewable energy?",
    "Should the research prioritize onshore wind power systems, offshore wind power systems, or both?"
  ],
  "followup_answers": [
    "I’m interested in solar panel efficiency.",
    "I want data on recent innovations.",
    "Focus on offshore wind power systems."
  ],
  "depth": 5,
  "breadth": 5
}

The backend first validates this request. It checks that the research_id matches an existing record from the /api/research/questions step, that the number of follow-up questions matches the number of answers, and that both depth and breadth are positive integers greater than zero. If any validation fails, the system returns an HTTP 400 Bad Request response with a detailed error message, such as "Depth must be a positive integer" or "Number of answers must match number of questions."
If the request is valid, the backend updates the database with the user’s answers, depth, and breadth under the corresponding research_id. This marks the beginning of the deep research process, which starts with generating the initial set of search queries.
Section 3: Query Generation Process
The deep research process begins with the generation of search engine queries, referred to as SERP (Search Engine Results Page) queries. This task is handled by an AI component called the Query Generator Agent. The agent’s job is to create queries that will be sent to a search engine API to retrieve relevant websites, and it operates in two distinct phases: generating top-level queries (Depth 1) and generating child queries for subsequent depths (Depth 2 and beyond).
For the top level, Depth 1, the Query Generator Agent analyzes the entire input provided by the user: the initial prompt, the follow-up questions, and the follow-up answers. Using this information, it generates exactly the number of SERP queries specified by the breadth parameter. In the example where breadth is 5, the agent produces five unique queries, each designed to explore a different angle or facet of the research topic. For instance, based on the user’s input focusing on solar panel efficiency, recent innovations, and offshore wind power systems, the agent might generate these Depth 1 queries:
"Latest advancements in solar panel efficiency 2023"

"Recent innovations in offshore wind power systems"

"New technologies improving solar panel efficiency"

"2023 breakthroughs in offshore wind energy"

"High-efficiency solar panels for renewable energy"

For each of these queries, the Query Generator Agent also generates a specific objective. The objective is a detailed instruction that defines what kind of information should be extracted from the websites returned by that query. For example, the query "Latest advancements in solar panel efficiency 2023" might have an objective like "Identify and extract specific data on the highest reported solar panel efficiency rates in 2023, including the technologies or materials used to achieve them." The objective ensures that the subsequent analysis of website content is targeted and relevant to the user’s research goals.
Each Depth 1 query, along with its objective, is stored in the database under the serp_queries field for the given research_id. The database entry for a query includes its text, its objective, its depth level (1 in this case), and a unique query ID. Since these are top-level queries, they have no parent query ID.
Once the Depth 1 queries are generated, the system begins processing them in parallel, as described in a later section. However, the query generation process doesn’t stop at Depth 1—it continues recursively for deeper levels based on the results of each query.
For depths beyond 1 (Depth 2 and onward), the Query Generator Agent generates child queries based on the outcomes of their parent queries. When a query at any depth completes—meaning all its associated websites have been scraped and analyzed—the system uses the extracted information from that query to inform the creation of new queries at the next depth. The number of child queries generated for each parent query is determined by a specific formula: Math.ceil(breadth / 2). This means the initial breadth value provided by the user is halved and rounded up to the nearest integer to determine the number of child queries per parent.
To illustrate with the example where breadth is 5: at Depth 1, there are 5 queries. When one of these Depth 1 queries completes, it generates Math.ceil(5 / 2) = Math.ceil(2.5) = 3 child queries at Depth 2. When one of those Depth 2 queries completes, it generates Math.ceil(3 / 2) = Math.ceil(1.5) = 2 child queries at Depth 3. This pattern continues until the specified depth (in this case, 5) is reached.
When generating child queries, the Query Generator Agent doesn’t just rely on the immediate parent query’s results. It considers the entire parent chain—that is, all queries from the current query back to its top-level ancestor at Depth 1. For example, to generate child queries at Depth 4, the agent uses the extracted content from the Depth 3 query, its Depth 2 parent, and its Depth 1 grandparent, alongside the initial prompt and follow-up Q&A. This cumulative context ensures that the research builds on all previous findings, becoming increasingly focused and detailed as it progresses deeper.
Suppose a Depth 1 query is "Latest advancements in solar panel efficiency 2023," and its analysis reveals a new material called perovskite improving efficiency. A Depth 2 child query might be "Perovskite solar panel efficiency improvements 2023," with an objective like "Extract data on how perovskite materials have increased solar panel efficiency, including specific efficiency percentages." If Depth 2 analysis mentions a company achieving 25% efficiency with perovskite, a Depth 3 child query might be "Company X perovskite solar panel 25% efficiency details," with an objective like "Find technical specifications and methods used by Company X to achieve 25% efficiency."
Each child query is assigned a unique query ID and linked to its parent query ID in the database, maintaining a clear tree structure of queries across all depths. This process repeats independently for each query branch until the maximum depth is reached.
Section 4: Processing SERP Queries
Once a SERP query is generated, the system processes it through a series of well-defined steps to gather and analyze information from relevant websites. This process applies to every query at every depth level and is designed to operate efficiently and robustly.
The first step is to retrieve website URLs for the query. The system sends the query text (e.g., "Latest advancements in solar panel efficiency 2023") to the searxng API, a search engine provider integrated into the backend. The API returns a list of website URLs that are deemed most relevant to the query, with a maximum limit of 7 URLs per query. For example, the response might include URLs like "https://example.com/solar-efficiency" and "https://technews.com/2023-solar-advances." These URLs are stored in the database under the query’s record, initially marked with a status of "pending."
Next, the system attempts to scrape the content of each URL in the list. Scraping involves sending an HTTP GET request to the website, retrieving its HTML content, and extracting the main textual content while discarding irrelevant elements like advertisements, navigation menus, or scripts. This is typically done using a scraping library that identifies the primary article or body text. For each URL, the system updates its status in the database to "scraping" while the process is underway. If scraping succeeds, the extracted text is stored alongside the URL, and the status is updated to "scraped." If scraping fails—perhaps due to a 403 Forbidden response, a timeout, or an unreadable page format—the system marks the URL’s status as "failed" and moves on to the next URL. Failed scrapes do not interrupt the process; the system simply proceeds with whatever websites it can successfully scrape.
After scraping, the system analyzes the content of each successfully scraped website using an AI component called the Website Analyzer Agent. This agent examines the extracted text in the context of the query’s specific objective. For instance, if the objective is "Identify and extract specific data on the highest reported solar panel efficiency rates in 2023, including the technologies or materials used to achieve them," the agent scans the text for relevant information. It might find a sentence like "In 2023, Company X achieved a record 25% efficiency with perovskite-based solar panels" and determine that this directly addresses the objective. The agent then extracts this sentence and ignores unrelated content, such as a paragraph about wind turbines on the same page.
The extracted information is stored in a JSON format that includes the relevant text and the exact URL for citation purposes. For example:
json

{
  "content": "In 2023, Company X achieved a record 25% efficiency with perovskite-based solar panels",
  "url": "https://example.com/solar-efficiency"
}

This JSON object is saved in the database under the successful_scraped_websites field for the query, and the URL’s status is updated to "analyzed." The Website Analyzer Agent processes each scraped website sequentially for a given query, ensuring that all relevant data is captured before the query is considered complete.
A query is deemed complete when all its URLs have been either successfully analyzed or marked as failed. At this point, the system triggers the generation of child queries (if the query is not at the maximum depth), as described in the previous section, and the process repeats for the next depth level.
Section 5: Massive Parallelization and Independent Depth Progression
To maximize efficiency, the system processes all queries in parallel wherever possible, allowing the research to progress as quickly as the available computational resources permit. This parallelization operates at both the top level and across all subsequent depths, with each query running independently of others.
At Depth 1, the system initiates processing for all top-level queries simultaneously. In the example where breadth is 5, the five queries (e.g., "Latest advancements in solar panel efficiency 2023," "Recent innovations in offshore wind power systems," etc.) are each assigned to a separate asynchronous task or thread. The backend uses an event-driven architecture or a task queue (e.g., implemented with a library like asyncio in Python or a worker system like Celery) to manage these tasks. Each task executes the full SERP query processing steps—retrieving URLs, scraping websites, and analyzing content—without waiting for the other Depth 1 queries to finish.
As soon as a query completes, it immediately generates its child queries for the next depth and begins processing them, regardless of the status of other queries at the same depth. For instance, if the query "Latest advancements in solar panel efficiency 2023" finishes analyzing its websites before "Recent innovations in offshore wind power systems," it generates its three Depth 2 child queries (per the Math.ceil(5 / 2) = 3 formula) and starts processing them right away. Meanwhile, the other Depth 1 queries continue their work independently. This ensures that the system is always making progress on some part of the research tree, rather than being bottlenecked by slower queries.
This independent progression applies recursively at all depths. For example, if a Depth 2 child query completes, it generates its own child queries for Depth 3 (e.g., Math.ceil(3 / 2) = 2 queries) and processes them immediately, even if other Depth 2 queries in its branch or elsewhere are still running. The only limit is the user-specified depth parameter—in this case, 5—which caps the recursion at Depth 5.
To manage the context for child query generation, the system passes the entire parent chain’s extracted content to the Query Generator Agent. For a Depth 4 query, this means collecting all JSON objects from the successful_scraped_websites field of the Depth 3 query, its Depth 2 parent, and its Depth 1 grandparent. These objects are combined into a single list and included in the input to the Query Generator Agent, along with the initial prompt and follow-up Q&A. This ensures that the agent has a complete picture of the research findings up to that point, enabling it to generate precise and contextually relevant child queries.
The database plays a critical role in tracking this parallel process. Each query’s status is updated as it progresses (e.g., "processing," "completed"), and the tree structure is maintained through parent-child relationships via query IDs. This allows the system to reconstruct the research progression and ensure that no branch exceeds the maximum depth.
Section 6: Report Generation Process
Once all queries across all depths have completed—meaning every website under every query has been scraped and analyzed or marked as failed—the system transitions to generating the final research report. This phase consolidates all the gathered data into a comprehensive document that addresses the user’s initial prompt and follow-up answers.
The process begins by collecting all extracted content from the successful_scraped_websites field across all queries in the database. This includes every JSON object containing the relevant text snippets and their corresponding URLs. For example, the collected data might include entries like:
json

[
  {"content": "In 2023, Company X achieved a record 25% efficiency with perovskite-based solar panels", "url": "https://example.com/solar-efficiency"},
  {"content": "Offshore wind systems saw a 15% increase in output due to new turbine designs", "url": "https://technews.com/2023-wind-advances"}
]

This data is then passed to an AI component called the Report Writing Agent, which is tasked with synthesizing the information into a coherent report. The agent starts by analyzing the initial prompt, the follow-up questions and answers, and the full set of extracted content to understand the scope and objectives of the research. It then constructs a detailed narrative that fully addresses every aspect of the user’s query.
The report is written in plain English but with a high level of technical detail, incorporating specific facts, figures, and quotes from the extracted content. For example, it might include a paragraph like this: "In 2023, significant advancements were made in solar panel efficiency, with Company X achieving a record 25% efficiency using perovskite-based materials. This breakthrough, reported on https://example.com/solar-efficiency, highlights the potential of perovskite to revolutionize solar energy production." Every factual claim or piece of information is directly tied to its source URL, ensuring that the report is fully cited and verifiable.
The Report Writing Agent does not rely on external knowledge or assumptions—it uses only the extracted content provided from the research process. This ensures accuracy and prevents the introduction of unverified information. The report is structured with clear sections and headings, such as "Advancements in Solar Panel Efficiency" and "Innovations in Offshore Wind Power," to make it easy to navigate, though the exact organization depends on the content and the user’s focus.
Once complete, the report is saved in the database under the report field for the research_id as a string (e.g., in Markdown format). The system marks the research process as finished, and the report becomes available for retrieval by the user through a separate API endpoint (not specified here, as it’s outside the core logic).
Section 7: Error Handling Mechanisms
To ensure the system remains robust, it includes error handling at key points in the process, allowing it to recover gracefully from failures without losing progress.
During website scraping, if a URL cannot be accessed or parsed—perhaps due to a server error, a paywall, or an incompatible format—the system logs the failure in the database by updating that URL’s status to "failed" with an error message (e.g., "HTTP 403 Forbidden"). It then skips that website and continues processing the remaining URLs for the query. This is not treated as a critical error, as the system can still produce valuable results with partial data.
For more severe errors, such as a bug in the code that prevents a query from processing or the report from being generated, the system takes a more comprehensive approach. It saves all progress to an error-output.md file associated with the research_id. This file includes a detailed summary of the research state at the time of failure: the full list of successfully scraped and analyzed websites with their extracted content and URLs, a list of websites that failed to scrape with their error messages, and any partial report content if the Report Writing Agent had begun its work. The file is written in Markdown for readability and stored in the database or filesystem, ensuring the user can access whatever was completed.
The database is updated progressively throughout the process—after generating follow-up questions, after generating each query, after scraping and analyzing each website, and after completing the report—so that the system can resume or audit the research if interrupted.
Section 8: Database Structure and Updates
All data generated during the research process is stored in a database following a fixed schema, ensuring consistency and traceability. The schema includes fields like these:
research_id: A unique string identifier for the research project (e.g., "abc123").

initial_prompt: The user’s original query as a string.

followup_questions: A list of strings containing the questions generated by the Prompt Analyzer Agent.

followup_answers: A list of strings containing the user’s responses.

depth: An integer specifying the maximum recursion level.

breadth: An integer specifying the number of top-level queries.

serp_queries: A list of objects, each representing a query with fields like query_id (a unique string), text (the query string), objective (the extraction instruction), depth (an integer), parent_query_id (a string or null for Depth 1), and status (e.g., "processing," "completed").

successful_scraped_websites: A list of objects, each with url (a string), status (e.g., "scraping," "analyzed," "failed"), content (the extracted text or null if failed), and error_message (a string or null if successful).

report: The final report as a string, or null if not yet generated.

The system updates the database at every significant step. After the /api/research/questions request, it saves the research_id, initial_prompt, and followup_questions. After the /api/research/start request, it adds the followup_answers, depth, and breadth. As each query is generated, it appends a new object to serp_queries. As each website is processed, it updates successful_scraped_websites with the latest status and content. Finally, when the report is generated, it populates the report field. This progressive updating ensures a complete record and supports error recovery.
Section 9: Absolute Implementation Requirements
To ensure the system functions as intended, the following requirements must be strictly followed in the codebase:
The depth and breadth parameters must be implemented exactly as specified. At Depth 1, the system generates breadth number of queries. From Depth 2 onward, each completed query generates Math.ceil(breadth / 2) child queries, with the number of child queries recalculated at each depth based on the previous level’s effective breadth. The recursion must stop at the user-specified depth, and each query must progress independently as soon as it completes, without waiting for peers at the same depth.
All data must be stored in the database according to the fixed schema, with updates made at every step to reflect the current state. This includes saving query details, website statuses, and extracted content immediately as they are generated, not in batches at the end.
Every piece of information in the report must be cited with its exact source URL. The Website Analyzer Agent must include the URL in every extracted content object, and the Report Writing Agent must reference these URLs for every factual claim, ensuring full traceability.
These requirements are non-negotiable and form the foundation of the system’s core logic.

