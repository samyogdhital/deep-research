Deep Research Codebase Requirements

This document provides a precise and comprehensive blueprint for building a highly autonomous deep research system. The system's purpose is to analyze a user's query, conduct extensive research across the internet, and produce a detailed, technically accurate report that fully answers the user's questions. Every step of the process is designed to be clear, efficient, and parallelized where possible, ensuring that the research is both broad and deep as specified by the user. The requirements are structured to leave no room for ambiguity, guiding an AI agent step-by-step through the implementation.

Purpose and Overview
The deep research system exists to help users explore topics in great detail by autonomously gathering and analyzing information from the internet. It begins with a user providing an initial prompt about a topic they want to research, along with details about their current knowledge and what they hope to learn. The system then generates follow-up questions to clarify the user's needs, which the user answers. Based on this input, along with user-specified parameters called breadth and depth, the system conducts a recursive research process. It generates search queries, retrieves websites, scrapes their content, extracts the most relevant information, and compiles everything into a comprehensive report. The report is packed with technical details, directly addresses the user's prompt and questions, and includes citations from every source used. The entire process is saved progressively in a database and updated in real-time to the frontend via websocket events, ensuring transparency and persistence.
The system is designed to handle research of any scale, whether the user wants a broad overview or a deep dive into specific aspects of a topic. Breadth controls how many initial search queries are generated, allowing the system to explore multiple angles of the topic simultaneously. Depth determines how many layers of follow-up research are conducted, enabling the system to build on previous findings and explore topics in greater detail. By parallelizing the processing of queries, the system ensures efficiency, allowing completed queries to move forward without delay while others finish.


User Input and Follow-Up Questions
The research process starts when the user submits an initial prompt to the system through an API endpoint called /api/research/questions. This prompt is a detailed description of the topic the user wants to investigate, including their current understanding and what they aim to achieve or learn. Along with the prompt, the user specifies how many follow-up questions they want the system to generate. This number is a key input, as it determines how much additional clarification the system will seek from the user.
Upon receiving this request, the backend processes the initial prompt using a prompt analyzer agent. This agent examines the prompt carefully to understand the user's intent and identify areas where more information is needed. It then generates the exact number of follow-up questions requested by the user. These questions are designed to extract as much relevant detail as possible, ensuring the system fully grasps what the user wants to explore. The backend resolves the API request by returning these follow-up questions to the frontend, where they are displayed to the user.
The user then answers each of the follow-up questions, providing detailed responses that further clarify their goals and scope. Once all answers are complete, the user submits this information, along with two new parameters—breadth and depth—to initiate the deep research process. This submission is sent to a second API endpoint called /api/research/start. The backend receives the initial prompt, the follow-up questions and answers, and the breadth and depth values, marking the official start of the research. These inputs form the foundation for all subsequent steps, guiding the system in generating queries and conducting research that precisely aligns with the user's needs.


Query Generation and Objectives
Once the deep research begins, a query generator agent takes over to create the initial set of search queries. This agent analyzes the user's initial prompt and the follow-up questions and answers to understand exactly what the user is seeking. For the first layer of research, known as depth 1, the agent generates a number of search queries equal to the breadth specified by the user. These are called top-level queries because they have no parent queries and serve as the starting point for the investigation. For example, if the user sets breadth to 5, the agent produces 5 top-level queries, each exploring a different angle of the topic based on the input provided.
Along with each query, the query generator agent defines a specific objective. The objective is a detailed statement of purpose that explains what kind of information the system should extract from the websites retrieved by that query. It acts as a guide for later steps, ensuring that only the most relevant content is collected. The agent crafts these objectives by deeply analyzing the user's prompt and answers, aiming to fill knowledge gaps and address the user's goals as precisely as possible. For instance, if the user wants to learn about recent advances in renewable energy, one query might focus on solar power innovations with an objective to "extract details about new solar panel technologies released in the past two years," while another might target wind energy with a different specific objective.
As the research progresses to deeper levels, the query generator agent continues to play a crucial role. After the top-level queries are fully processed, it generates child queries for the next depth layer based on the findings from the previous layer. The number of child queries for each parent query is calculated as Math.ceil(breadth / 2), meaning the breadth is halved and rounded up. So, with a breadth of 5, each completed top-level query spawns Math.ceil(5 / 2) = 3 child queries at depth 2. To create these child queries, the agent considers the user's initial prompt, the follow-up questions and answers, and all the learnings extracted from the parent query's websites. It also traces the full chain of parent queries back to the top level, ensuring that every previous layer's insights inform the new queries. This recursive process repeats for each depth level specified by the user, building a progressively deeper understanding of the topic.


Search, Scraping, and Website Analysis
With the queries and objectives in hand, the system begins processing them by sending each query to SearxNG, a privacy-focused search engine. For every query, SearxNG returns a list of up to 7 top URLs that are most likely to contain relevant information. These websites become the targets for the next step: scraping. The system attempts to download and extract the textual content from each of these websites. However, scraping may not always succeed due to issues like paywalls or technical restrictions. If a website cannot be scraped, the system logs the failure and moves on to the next one without halting the process.
For every website that is successfully scraped, the system invokes a website analyzer agent. This agent is responsible for examining the scraped content and pulling out the most valuable information that aligns with the objective tied to the query. The agent reads through the text, identifies passages, facts, or data that directly contribute to fulfilling the objective, and discards anything irrelevant. For example, if the objective is to find recent solar panel technologies, the agent might extract a paragraph describing a new prototype while ignoring unrelated sections about company history. The extracted content is stored in a structured format, including the exact URL of the website and specific quotes or figures used, ensuring that every piece of information can be traced back to its source.
This process happens for every website under every query, and it’s where parallelization becomes critical. At depth 1, all top-level queries—equal to the breadth—are processed at the same time. The system doesn’t wait for one query to finish before starting another; instead, it sends all queries to SearxNG concurrently, scrapes the resulting websites in parallel, and runs website analyzer agents simultaneously for each successful scrape. Once a top-level query is fully processed—meaning all its websites are scraped and analyzed—it moves to the next depth independently, without waiting for its sibling queries to complete. This parallel approach ensures that the research progresses as quickly as possible, maximizing efficiency.


Depth and Breadth Mechanics
The breadth and depth parameters shape the entire research process, controlling how wide and how deep the investigation goes. Breadth sets the number of top-level queries at depth 1, determining how many different perspectives the system explores initially. Depth dictates how many layers of recursive research are performed, allowing the system to refine and expand its findings over multiple rounds. Together, these parameters create a structured yet flexible framework for the research.
At depth 1, the system generates a number of queries equal to the breadth. For instance, if the user specifies a breadth of 2 and a depth of 3, the process starts with 2 top-level queries. Each of these queries is processed fully—sent to SearxNG, websites retrieved, scraped, and analyzed—before moving forward. Because the system parallelizes these top-level queries, one might finish faster than the other, but both must complete their full cycle. Once a query is done, it immediately spawns child queries for depth 2. The number of child queries per parent is Math.ceil(breadth / 2), so with a breadth of 2, each top-level query generates Math.ceil(2 / 2) = 1 child query. This means depth 2 would have 2 child queries total (1 from each of the 2 top-level queries).
For each child query at depth 2, the process repeats: the query is sent to SearxNG, websites are scraped, and website analyzer agents extract relevant content based on the new objective. Once these child queries are fully processed, they spawn their own child queries for depth 3, again using Math.ceil(breadth / 2) = 1 query per parent, resulting in 2 queries at depth 3. This continues until the specified depth is reached. In this example, with a depth of 3, the process stops after depth 3 is complete, having explored 2 top-level queries, 2 depth 2 queries, and 2 depth 3 queries, for a total of 6 queries.
The key to this recursive flow is how child queries are generated. When creating queries for a new depth, the query generator agent doesn’t just look at the immediate parent query’s results. It considers the entire chain of parent queries back to the top level, along with the user’s initial prompt and follow-up answers. This ensures that each new layer builds on all previous learnings, filling in gaps and diving deeper into the topic. Parallelization enhances this process by allowing a query to advance to its next depth as soon as it’s ready, without waiting for unrelated queries at the same level to finish. This dynamic progression keeps the research moving forward efficiently while maintaining a thorough exploration.
The core logic you need to keep in mind with example.
for depth 3 and breath 3 query, it should be total 15 serp queries.
for depth 5 and breadth 5 query, it must be total 110 serp queries.
for depth 2 and breadth 2 query, it must be total 4 serp queries.
for depth 2 and breadth 4 query, it must be total 12 serp queries. 
for depth 4 and breath 2 query, it must be total 8 serp queries. 



Report Generation
After all queries at all depths are fully processed—meaning every website has been scraped and analyzed—the system shifts to generating the final report. This step begins only when every query has completed its cycle, ensuring that no information is missed. The report generation process involves compiling all the extracted content from every website analyzer agent into a single, cohesive document that directly addresses the user’s initial prompt and follow-up questions.
To create the report, the system analyzes all the collected data, synthesizing it into a narrative that answers the user’s queries comprehensively. The report is designed to be highly detailed and technically accurate, packed with valuable insights and free of fluff. Every statement, argument, or conclusion in the report is tied directly to the extracted content, and at the end of each sentence, the system cites the exact website URL(s) from which the information was sourced. If a point draws from multiple websites, all relevant URLs and specific quotes or facts are included in the citation. This meticulous citation process ensures that the user can trace every piece of information back to its origin, enhancing credibility and transparency.
The report is structured logically, with sections and subsections that reflect the scope and nuances of the user’s request. For example, if the user asked about renewable energy advances, the report might have sections on solar power, wind energy, and emerging technologies, each filled with precise details and examples drawn from the research. The goal is to leave no question unanswered, covering every aspect the user wanted to explore in a way that’s both accessible and deeply informative. Once the report is complete, it’s saved to the database and sent to the frontend, marking the end of the research process.


Error Handling
The system is built to be robust, handling potential failures without crashing the entire process. Errors can occur at various stages, such as a website failing to scrape or an API call timing out. If a website cannot be scraped, the system logs the failure and continues with the remaining websites for that query, treating it as a minor issue rather than a critical stop. However, if a major error occurs—like a complete failure of the search engine API or an inability to generate the report—the system takes more significant action.
In the case of a critical error that halts the research, the system generates a file called error-output.md. This file contains a detailed summary of what went wrong, including the specific error message and its impact. It also includes all the progress made up to that point: a list of successfully scraped websites with their extracted content and citations, a list of websites that failed to scrape with reasons if available, and any other relevant data collected so far. This ensures that even if the research cannot be completed, the user still receives partial results and an explanation of the issue. The error file is saved to the database and sent to the frontend via a websocket event, maintaining consistency with the system’s real-time update approach.


Database and Real-Time Updates
The system uses a fixed database schema defined in the response-schema.ts file to store all data throughout the research process. This schema is a strict blueprint that cannot be altered, ensuring that every piece of information is saved in a consistent format. As each step occurs—generating follow-up questions, creating queries, scraping websites, analyzing content, or writing the report—the system updates the database immediately. For example, when a query retrieves a list of websites, those URLs are saved right away. When a website is scraped and analyzed, the extracted content and citations are added to the database instantly. This progressive updating means that the database always reflects the current state of the research, even if the process is interrupted.
Every time the database is updated, the system fires a websocket event to send the latest data to the frontend. Before firing the event, the system ensures that the update is fully saved to the database. Then, it retrieves the entire current database state for that research process—identified by a unique research ID—and includes it in the websocket payload. This payload follows the fixed schema, providing a complete snapshot of the research at that moment. The frontend receives these updates in real-time, allowing users to see the progress as it happens, whether it’s a new query being generated or a website being analyzed. This seamless synchronization keeps the user informed and ensures that the research persists across browser sessions.
The specific websocket events fired during the process are carefully defined to match each stage of the research. During the research start process, the system fires generating_followups when it begins creating follow-up questions and followups_generated when they’re ready. For the information gathering process, it fires new_serp_query when a query is generated, got_websites_from_serp_query when the website list is retrieved, scraping_a_website for each website as scraping starts, analyzing_a_website as analysis begins, and analyzed_a_website when analysis is complete. For the report writing process, it fires report_writing_start when compilation begins and report_writing_successful when the report is finished. Each of these events is tied to a specific action, saved to the database first, and then sent with the full database snapshot, ensuring precise and consistent communication.


Frontend Interaction and Display
The frontend is designed to provide a smooth, real-time experience for the user, reflecting the backend’s progress through websocket events. When the user submits their initial prompt and follow-up question count to /api/research/questions, the backend returns the questions, which the frontend displays in a form. After the user fills in the answers and submits them with breadth and depth to /api/research/start, the deep research begins, and the frontend starts receiving websocket updates. These updates drive the UI, showing the research process as it unfolds.
The sidebar on the frontend is split into two main sections: one for past research reports and one for ongoing research. Past reports are organized by time periods like "Today" or "Previous 7 Days," allowing users to revisit completed work. The "Ongoing Research" section shows active research processes, each represented by a skeleton loader—a placeholder that matches the size and shape of a report title. The number of skeleton loaders corresponds to the number of active research processes, ensuring the user can see everything currently running. If a user opens a new tab or browser session, the frontend retrieves the full history of events for each ongoing research from the database via the websocket connection, displaying both past logs and live updates seamlessly.
When the user clicks on a skeleton loader, the UI switches to show the real-time logs for that specific research process. These logs include every websocket event fired so far—such as new_serp_query or analyzed_a_website—along with details like which websites were scraped and what content was extracted. The logs update dynamically as new events arrive, giving the user a clear view of the research’s progress. Importantly, logs are isolated to the selected research process; switching to a different loader shows only that process’s logs, preventing any mix-up. This persistence and isolation ensure that users can monitor long-running research (which might take 1-2 hours) from any tab, picking up exactly where they left off.
Conclusion
This deep research codebase is a powerful tool for autonomous, in-depth investigation, guided by the user’s initial prompt, follow-up answers, and breadth and depth parameters. It generates precise queries with clear objectives, processes them in parallel across multiple depths, and extracts valuable content from websites to build a comprehensive, cited report. Every step is saved to a fixed database schema and synced with the frontend in real-time via websocket events, ensuring transparency and reliability. By following this detailed, deterministic specification, an AI agent can implement a system that delivers high-quality research efficiently, meeting the user’s needs with clarity and precision.